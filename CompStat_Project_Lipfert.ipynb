{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dimensionality Reduction and Empirical Application\n",
    "\n",
    "### A Project by Marc Lipfert, Matriculation Number: 3220513\n",
    "\n",
    "### Computational Statistics (Summer Semester 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[1.) Introduction](#introduction)\n",
    "\n",
    "2.) Dimensionality Reduction Using Principal Component Analysis (PCA)\n",
    "\n",
    "3.) Outline of the Empirical Setting\n",
    "\n",
    "4.) Comparison of PCA and Factor Analysis\n",
    "\n",
    "5.) Simulation Study\n",
    "\n",
    "6.) Empirical Application to Actual Survey Data\n",
    "\n",
    "7.) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1.) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pca'></a>\n",
    "## 2.) Dimensionality Reduction Using Principal Component Analysis (PCA)\n",
    "\n",
    "This section aims to introduce Principal Component Analysis (PCA) and thereby intends to answer the following questions: In which contexts is PCA a useful tool? And what are its properties and how is it conducted? \n",
    "\n",
    "*(a) In which contexts is PCA a useful tool?*\n",
    "\n",
    "PCA is essentially a dimensionality reduction method. In the case of a data set with many variables (\"high dimensionality\"), an empirical model based on an extensive set of variables is usually difficult to interpret. Further, the predictive performance of such a model may be poor due to overfitting. Therefore, the aim is often to create a small(er) set of new variables which captures most of the statistical information contained in the original variables. PCA intends to achieve exactly this, where those newly generated variables are called *Principal Components* and are linear combinations of the original variables. \n",
    "\n",
    "To illustrate in more detail what this means and why it is done, it is useful to fix ideas and draw on a formal framework. The set-up is intended to resemble a situation that is frequently encountered in microeconometrics. Assume that a survey is conducted among randomly selected individuals. The sample size is $N$ and the econometrician is interested in the relationship between the outcome $Y$ and a set of $P$ explanatory variables $X_1 , \\ldots , X_P$. The empirical model (structural equation) for an indivdidual $i$ is assumed to be as follows:\n",
    "\n",
    "$$ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\ldots + \\beta_P X_{Pi} + \\epsilon_i \\text{,} $$\n",
    "\n",
    "where $\\epsilon$ denotes an error term that is assumed to have a conditional expectation of zero: $E[\\epsilon | X] = 0$. Using matrix notation, $ \\pmb Y$ and $\\pmb \\epsilon$ denote column vectors each of length $N$ and $\\pmb X$ is a $N \\times (P + 1)$ matrix. Then, the re-written structural equation reads: \n",
    "\n",
    "$$ Y = \\pmb X \\beta + \\epsilon \\text{ , where } \\pmb X = \\begin{pmatrix} 1 & X_{11} & X_{21} & \\dots & X_{P1} \\\\\n",
    "1 & X_{12} & X_{22} & \\dots & X_{P2} \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & X_{1N} & X_{2N} & \\dots & X_{PN} \\end{pmatrix} $$. \n",
    "\n",
    "The task of the econometrician is to find estimates $\\hat{\\beta}$ of the population parameters $\\beta$, usually obtained by minimizing the sum of squared residuals (i.e. OLS).  \n",
    "\n",
    "There are two problems that go along with high dimensional data, i.e. large $P$. First, when $N$ is only slightly larger than $P$, *overfitting* is likely to occur. In such a setting with a large number of explanatory variables relative to the sample size, the estimated coefficients of the explanatory variables might pick up on random noise rather than capturing the (\"true\") structural parameters. This can be viewed as an instance of the bias-variance trade-off: given standard assumptions, the numerous estimated coefficients would be unbiased but the variance of the obtained predictions would be large and thus highly dependent on the random sample that the model is fit on. Put differently, the estimated function $\\hat{f}$ with coefficients $ \\hat{\\beta}$ would yield poor predictions (or respectively, high prediction errors) when being applied to a different random sample of the population (\"test data set\").\n",
    "\n",
    "The second issue in the context of high dimensional data is the limited interpretability. Given the large number of explantory variables, a meaningful interpretation of the model may no longer be feasible. One might be inclined to interpret the estimated coefficients separately - but since high dimensionality can go along with large correlations among the explanatory variables, the issue of multicollinearity may render this approach useless as it cannot be pinned down which variable is truely predictive of the outcome.\n",
    "\n",
    "Dimensionality reduction can be employed to address those two issues. Principal Component Analysis (PCA) is probably the most common method to achieve this. As already stated, it entails finding linear combinations of the explanatory variables that encapsulate as much of the statistical information (\"variability\") contained in the original variables as possible. The actual procedure is outlined in the following.\n",
    "\n",
    "*(b) What are the properties of PCA and how is it conducted?*\n",
    "\n",
    "To begin with, it is common practice to centre (or \"demean\") the explanatory variables, which is done as follows: \n",
    "\n",
    "$$ \\tilde{x}_{ij} = x_{ij} - \\bar{x}_j \\text{,}$$\n",
    "\n",
    "where $\\bar{x}_j$ denotes the sample mean of the $j$th explanatory variable across the $N$ individuals. Importantly, this does not alter the solution besides centring it at zero. Note that when the units of measurement differ across the explanatory variables, e.g. height in cm and temperature in degree Celsius, it is advisable to go even further and standardise the variables to mean zero and standard deviation of 1. This is because of an undesirable feature that PCA has which is the sensitivity of the results to the units of measurement. A change in the units of measurement of a variable also affects its variance - and as the principal components are constructed such that they capture the greatest possible variability of the original variables, it follows that the principal components would change. This issue of standardisation will be picked up on in the following section.\n",
    "\n",
    "We are then interested in obtaining linear combinations $Z_1, \\dots, Z_M$ of the centred variables, where $M \\leq P$. Those linear combinations are referred to as prinicpal components and they are ordered according to their variance in a descending fashion, thus the first principal component $Z_1$ is the linear combination of the centred explanatory variables,\n",
    "\n",
    "$$ Z_1 = \\phi_{11} \\tilde{X}_1 + \\phi_{12} \\tilde{X}_2 + \\dots + \\phi_{1P} \\tilde{X}_P = \\sum_{j=1}^P \\phi_{1j} \\tilde{X}_j = \\tilde{\\pmb X} \\phi_1 \\, \\text{ ,}$$\n",
    "\n",
    "that yields the highest variance. The scalars $\\phi_{11}, \\phi_{12}, \\dots, \\phi_{1P}$ are referred to as *loadings* of the first principal component and $\\phi_1$ denotes the corresponding column vector $\\phi_1 = (\\phi_{11} \\, \\phi_{12}  \\, \\dots \\,  \\phi_{1P})^T$. \n",
    "\n",
    "To derive the actual loadings of the first principal component, we have to note that any such linear combination has its variance given by $Var(\\tilde{\\pmb X} \\phi)  = \\phi^T \\pmb S \\phi$, where $\\pmb S$ denotes the sample covariance matrix of the (centred) explanatory variables. Thus, the vector $\\phi_1$ must be obtained, which maximises the quadratic form $\\phi^T \\pmb S \\phi$. To ensure a well-defined solution to this problem, $\\phi$ is required to be a unit-norm vector, i.e. $\\phi^T \\phi = 1$. Put together, the constrained maximisation problem can be written as \n",
    "\n",
    "$$\\max_{\\phi} \\phi^T \\pmb S \\phi - \\lambda (\\phi^T \\phi -1) \\text{  .}$$\n",
    "\n",
    "Solving this problem yields:\n",
    "\n",
    "$$ \\pmb S \\phi - \\lambda \\phi = \\pmb 0 \\iff  \\pmb S \\phi = \\lambda \\phi \\text{  .}$$\n",
    "\n",
    "Thus, $\\phi$ is an eigenvector of the covariance matrix $\\pmb S$ and the scalar $\\lambda$ denotes the corresponding eigenvalue. Since we are interested in the linear combination with the largest variance  $Z_1$, the respective loadings vector $\\phi_1$ (i.e. the eigenvector) is identified by the largest eigenvalue $\\lambda_1$ as \n",
    "\n",
    "$$ Var(\\tilde{\\pmb X} \\phi)  = \\phi^T \\pmb S \\phi = \\lambda \\phi^T \\phi = \\lambda \\, \\text{ ,}$$\n",
    "\n",
    "using the solution of the maximisation problem. Note that this solution of the maximisation problem is unaffected if the eigenvectors $\\phi$ is multiplied by (-1), so the actual signs of the loadings are meaningless - merely their respective magnitudes as well as their sign patterns matter. \n",
    "\n",
    "As $\\pmb S$ is a symmetric $P \\times P$ matrix, a total number of $P$ eigenvalues can be obtained. The corresponding eigenvectors $\\phi_j$ with $j=1, \\dots, P$ can be required to be orthogonal to each other, which is ensured when they satify $\\phi_{j}^T \\phi_{j\\,'} = 1$ for $j=j\\,'$ and zero otherwise. It can be shown that the remaining $2, \\dots, P$ linear combinations can be found as solutions to the problem of successively maximizing the variance subject to the constraint of uncorrelatedness with previously obtained linear combinations.\n",
    "\n",
    "In consequence, we are able to obtain $P$ linear combinations, i.e. principal components, of the $P$ centred explanatory variables - and all of these linear combinations are uncorrelated. Further, they are arranged in descending order according to their variance, which implies that the variablility of the original variables is more concentrated within the first couple of principal components. Dimensionality reduction now means that not all of the principal components are used (otherwise no dimensionality reduction would have been achieved) but instead only the principal components up to $M < P$ are utilised. In a regression framework, this implies conducting Principal Component Regression (PCR): instead of regressing the outcome variable $Y$ on the original explanatory variables $\\pmb X$, we regress it on a number of principal components:\n",
    "\n",
    "$$ Y_i = \\gamma_0 + \\gamma_1 Z_{1i} + \\dots + \\gamma_M Z_{Mi} \\, \\text{ ,}$$\n",
    "\n",
    "The crucial assumption thereby is that the directions in which the (centred) original variables have the most variability are also the directions that are associated with the outcome variable $Y$. The word \"direction\" is used as the vectors of principal component loadings, $\\phi$, characterise a direction in a multi-dimensional space. Given that this assumption holds, using only $M$ linear combinations that contain most of the original statistical information improves upon the overfitting problem if $M << P$. Referring back to the variance-bias trade-off, we accept that a small part of the original statistical information is not used (i.e. introducting bias) while achieving a reduction of the variance of our predictions when being applied to a new random sample (test data set) which is likely to overcompensate for the introduced bias.  Further, interpretatibility of the model is increased as only a lower number of variables is included - which are also uncorrelated, so multicollinearity is no longer an issue.\n",
    "\n",
    "The questions remains, how many principal components should be included in the regression model. A helpful concept in this context is the *Proportion of Variance Explained* (PVE). As already stated, we intend to reduce the dimensionality of the data while preserving as much as possible of the original variability. So the PVE states how much of the total variance is captured by an individual principal component. As the explanatory variables are centred, the total variability is calculated as follows:\n",
    "\n",
    "$$ \\sum_{j=1}^{P} Var(\\tilde{X}_j) = \\sum_{j=1}^{P} \\frac{1}{N}\\sum_{i=1}^{N} \\tilde{x}_{ji}^2 \\text{ .}$$\n",
    "\n",
    "Further, the variance attributable to the $m$th principal component is \n",
    "\n",
    "$$ \\frac{1}{N} \\sum_{i=1}^{N} z_{mi}^2 = \\frac{1}{N} \\sum_{i=1}^{N} (\\sum_{j=1}^{P} \\phi_{mj} \\tilde{x}_{ji})^2 \\text{ .}$$\n",
    "\n",
    "Hence, the the proportion of total variance explained by the principal component $m$ is given by:\n",
    "\n",
    "$$ \\frac{\\sum_{i=1}^{N} (\\sum_{j=1}^{P} \\phi_{mj} \\tilde{x}_{ji})^2}{\\sum_{j=1}^{P} \\sum_{i=1}^{N} \\tilde{x}_{ji}^2} \\text{ .}$$\n",
    "\n",
    "Each PVE of the individual principal components is a positive number that lies between 0 and 1. In order to obtain the *cumulative PVE* up to prinipal component $M$, this would simply be the sum over the PVE of components $1, \\dots, M$. Both, the PVE and the cumulative are often visualized, where especially plotting the former against the number of principal components receives particular attention and is know as a \"Scree Plot\" (an example will be presented later).\n",
    "\n",
    "To pin down the number of principal components to include in a PCR, a number of rather *ad-hoc* decision rules have been proposed. For instance, choosing only as many principal components as to explain a sizeable fraction of the total variance has been suggested. While it is of course context-dependent, which actual cut-off with respect to the cumulative PVE is appropriate, requiring the principal components to explain at least 70-90\\% of the total variance can be seen as common practice (see Jolliffe, 112-113). A more visual approach would be to analyse the Scree Plot with the PVE on the y-axis and the number of principal components on the x-axis. By eyeballing one would identify the last principal component after which the PVE of the following ones drops off, which is sometimes referred to as an *elbow* (Hastie/Tibshirani, 384). A last example of those rather ad-hoc decision rules would be to keep those principal components with a corresponding eigenvalue $\\lambda$ that is larger than the average eigenvalue $\\bar{\\lambda}$, or only larger than $0.7 \\bar{\\lambda}$ (Jolliffe, 115).\n",
    "\n",
    "A more systematic and, as some would argue, more objective way to identify the number of principal components to include is to choose them via *Cross-Validation*. As already stated, our aim is to identify an empirical model which is able to explain $Y$ well. Thus, a well fitted model should be able to give good predictions of the outcome when being applied to different data that is likewise randomly drawn from the population. Overfitting is a potential issue which could lead to a situation in which the model is able to predict $Y$ well on a data set the model was trained on, but does poorly in predicting the outcome when using a test data set. One way to achieve good predictive performance on test data is to use Cross-Validation (CV). Then, the available data is randomly split into $k$ folds, commonly $k = 10$. Initially, the first fold is kept as a validation set, and the model is fit on the rest of the data, i.e. on the other $(k-1)$ folds. The quality of the estimated model $\\hat{f}$ is evaluated on the held-out fold and the corresponding prediction error is calculated as the mean squared error (MSE). This procedure is repeated $k$ times, always keeping another fold as the validation set. The $k$-fold Cross-Validation estimate of the MSE is then simply the average of the obtained measures of the prediction errors: $MSE_1, \\dots, MSE_k$. Referring back to the problem of selecting the appropriate number of principal components, the number would be chosen in order to achieve the lowest CV MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1\n",
    "### Visualisation of the General Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach is best described when starting with low-dimensional data, such as $P = 2$, as this can easily be visualised. For this purpose, data is drawn from a bivariate normal distribution in which the data generating process features both variables to have differing variances. Further, they have a positive covariance. The centred data as well as the two principal components are presented in the following graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'mvtnorm' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'gridExtra' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'cowplot' was built under R version 3.6.3\"\n",
      "********************************************************\n",
      "Note: As of version 1.0.0, cowplot does not change the\n",
      "  default ggplot2 theme anymore. To recover the previous\n",
      "  behavior, execute:\n",
      "  theme_set(theme_cowplot())\n",
      "********************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(mvtnorm)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(cowplot)\n",
    "\n",
    "#adjust size of graphs\n",
    "options(repr.plot.res = 80) \n",
    "# options(repr.plot.width=4, repr.plot.height=4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prco <- function(X) {\n",
    "    cov <- cov(X)\n",
    "    p <- dim(cov)[1]\n",
    "    \n",
    "    pca <- eigen(cov)\n",
    "    colnames(pca$vectors) <- paste(\"PC\", 1:p, sep=\"\")\n",
    "    rownames(pca$vectors) <- paste(\"X\", 1:p, sep=\"\")\n",
    "    \n",
    "    PVE <- pca$values / sum(apply(X, 2, var))\n",
    "    cPVE <- cumsum(PVE)\n",
    "    importance <- round(rbind(pca$values, PVE, cPVE), 4)\n",
    "    rownames(importance)[1] <- \"Var (Eigenval.)\"\n",
    "    colnames(importance) <- paste(\"PC\", 1:p, sep=\"\")\n",
    "    \n",
    "    vars <- cbind(paste(\"PC\", 1:p, sep=\"\"), pca$values)\n",
    "    colnames(vars) <- c(\"PC\", \"var\")\n",
    "    df_vars <- data.frame(vars)\n",
    "    df_vars$var <- as.numeric(as.character(df_vars$var))\n",
    "    \n",
    "    return(list(values=pca$values, vectors=pca$vectors, df_vars = df_vars, importance=importance))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAIwCAMAAACvL6FdAAAAGFBMVEUAAAAAAP8zMzNNTU3r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\nsQRgIJYADMQSgIFY+h/4WKgOqDFV4wAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bivariate Normal Distribution\n",
    "\n",
    "set.seed(1234)\n",
    "\n",
    "N <- 200\n",
    "sigma <- matrix(c(5, 6, 6, 12), byrow = T, nrow = 2)\n",
    "mu <- c(5, 15)\n",
    "\n",
    "X <- rmvnorm(N, mean = mu, sigma = sigma)\n",
    "colnames(X) <- c(\"X1\", \"X2\")\n",
    "X_demeaned <- scale(X, center=T, scale = F)\n",
    "\n",
    "df_demeaned <- as.data.frame(X_demeaned)\n",
    "\n",
    "pca <- prco(X_demeaned)\n",
    "\n",
    "p1 <-   ggplot(df_demeaned, aes(X1, X2)) + \n",
    "        xlim(-10, 10) +\n",
    "        geom_point() + \n",
    "        geom_abline(aes(slope = pca$vectors[2, 1] / pca$vectors[1, 1], intercept = 0, colour=\"PC1\")) + \n",
    "        geom_abline(aes(slope = pca$vectors[2, 2] / pca$vectors[1, 2], intercept = 0, colour=\"PC2\")) +\n",
    "        theme(plot.title = element_text(hjust = 0.5)) +\n",
    "        labs(color = \"\") + xlab(\"X1 (demeaned)\") + ylab(\"X2 (demeaned)\") +\n",
    "        scale_color_manual(values = c(\"PC1\" = \"red\", \"PC2\" = \"blue\")) +\n",
    "        ggtitle(\"Visualisation of Principal Components\") +\n",
    "        coord_fixed()\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive covariance of the data can be spotted as the point cloud has an upward slope. Since the first principal component is by definition the linear combination of the two variables which captures as much of their variability as possible, it also has a positive slope, thereby capturing the co-movement. The first principal component, depicted in the graph by the red line, has the property that it is the line which minimises the sum of the squared perpendicular distances between the data points and the line. The second principal component (blue line), by construction, must be orthogonal to the first one. Thus, in the two-dimension space, it is perpendicular to the first one. As can already been seen from the graph, the variability in the direction of the first principal component is in this case distinctly larger than the one in the direction of the second one. \n",
    "\n",
    "The numerical values of the principal component loadings are presented below. The first principal component assigns more weight to the second variable X2 than to the first variable X1 (0.86 vs. 0.5), as the former is also the one with a higher variance (see sample covariance matrix below). And indeed, the first principal component captures far more of the total variability than the second one: about 92\\% of the total variability is explained by it and only around 8\\% by the second component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Sample Covariance Matrix:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>5.3894 </td><td> 6.6935</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>6.6935 </td><td>13.0390</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X1 & X2\\\\\n",
       "\\hline\n",
       "\tX1 & 5.3894  &  6.6935\\\\\n",
       "\tX2 & 6.6935  & 13.0390\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1 | X2 |\n",
       "|---|---|---|\n",
       "| X1 | 5.3894  |  6.6935 |\n",
       "| X2 | 6.6935  | 13.0390 |\n",
       "\n"
      ],
      "text/plain": [
       "   X1     X2     \n",
       "X1 5.3894  6.6935\n",
       "X2 6.6935 13.0390"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Principal Components:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>0.5019 </td><td>-0.8649</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>0.8649 </td><td> 0.5019</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tX1 & 0.5019  & -0.8649\\\\\n",
       "\tX2 & 0.8649  &  0.5019\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| X1 | 0.5019  | -0.8649 |\n",
       "| X2 | 0.8649  |  0.5019 |\n",
       "\n"
      ],
      "text/plain": [
       "   PC1    PC2    \n",
       "X1 0.5019 -0.8649\n",
       "X2 0.8649  0.5019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Importance of Components:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Var (Eigenval.)</th><td>16.9235</td><td>1.5050 </td></tr>\n",
       "\t<tr><th scope=row>PVE</th><td> 0.9183</td><td>0.0817 </td></tr>\n",
       "\t<tr><th scope=row>cPVE</th><td> 0.9183</td><td>1.0000 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tVar (Eigenval.) & 16.9235 & 1.5050 \\\\\n",
       "\tPVE &  0.9183 & 0.0817 \\\\\n",
       "\tcPVE &  0.9183 & 1.0000 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| Var (Eigenval.) | 16.9235 | 1.5050  |\n",
       "| PVE |  0.9183 | 0.0817  |\n",
       "| cPVE |  0.9183 | 1.0000  |\n",
       "\n"
      ],
      "text/plain": [
       "                PC1     PC2   \n",
       "Var (Eigenval.) 16.9235 1.5050\n",
       "PVE              0.9183 0.0817\n",
       "cPVE             0.9183 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAIwCAMAAACvL6FdAAAAElBMVEUAAAAA/wAzMzNNTU3r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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Sample Covariance Matrix:\")\n",
    "# Sample cov is invariant to demeaning\n",
    "round(cov(df_demeaned), 4)\n",
    "\n",
    "print(\"Principal Components:\")\n",
    "round(pca$vectors, 4)\n",
    "\n",
    "print(\"Importance of Components:\")\n",
    "pca$importance\n",
    "\n",
    "ggplot(data = pca$df_vars, aes(x=PC, y=var)) +\n",
    "    geom_bar(stat=\"identity\", fill=\"green\") + ylab(\"Variance (Eigenvalue)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, it shall be demonstrated that PCA with standardised data indeed leads to different principal components than when using data that is merely demeaned. The following plot shows the demeaned data from before on the left side and the standardised version of the same underlying data on the right. As one can see, the slope of the principal components differ across panels. The first principal component with demeaned data assigned more weight to the second variable X2, which is captured by the corresponding red line having a slope larger than one. With standardised data, the first principal component has identical factor loadings for the two variables as the differences in variance across the variables are now levelled.\n",
    "\n",
    "This is because standardisation alters the covariance matrix while demeaning leaves is unaffected. Since the principal components are obtained as the eigenvectors of the covariance matrix, they then also differ. The covariance matrix of the standardised data is in fact the correlation matrix of the underlying data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAGFBMVEUAAAAAAP8zMzNNTU3r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\nQgAUhBAABSEE8H9inUxNV4iaBQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_std <- scale(X, center = T, scale = T)\n",
    "df_std <- as.data.frame(X_std)\n",
    "pca_std <- prco(X_std)\n",
    "\n",
    "p2 <-   ggplot(df_std, aes(X1, X2)) + \n",
    "        geom_point() + \n",
    "        geom_abline(aes(slope = pca_std$vectors[2, 1] / pca_std$vectors[1, 1], intercept = 0, colour=\"PC1\")) + \n",
    "        geom_abline(aes(slope = pca_std$vectors[2, 2] / pca_std$vectors[1, 2], intercept = 0, colour=\"PC2\")) +\n",
    "        theme(plot.title = element_text(hjust = 0.5)) +\n",
    "        labs(color = \"\") + xlab(\"X1 (stand.)\") + ylab(\"X2 (stand.)\") +\n",
    "        scale_color_manual(values = c(\"PC1\" = \"red\", \"PC2\" = \"blue\")) +\n",
    "        ggtitle(\"PCA with standardised data\") +\n",
    "        coord_fixed()\n",
    "\n",
    "p1 <- p1 + ggtitle(\"PCA with demeaned data\")\n",
    "\n",
    "options(repr.plot.width=10, repr.plot.height=8)\n",
    "grid.arrange(p1, p2, nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the correlation matrix is displayed. Both variables have a high correlation of about .8 and this leads to factor loadings of the first principal component that equally weight both of the standardised variables. Still, the first component explains about 90\\% of the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Sample Covariance Matrix:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>5.389441 </td><td> 6.693512</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>6.693512 </td><td>13.039028</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X1 & X2\\\\\n",
       "\\hline\n",
       "\tX1 & 5.389441  &  6.693512\\\\\n",
       "\tX2 & 6.693512  & 13.039028\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1 | X2 |\n",
       "|---|---|---|\n",
       "| X1 | 5.389441  |  6.693512 |\n",
       "| X2 | 6.693512  | 13.039028 |\n",
       "\n"
      ],
      "text/plain": [
       "   X1       X2       \n",
       "X1 5.389441  6.693512\n",
       "X2 6.693512 13.039028"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Covariance Matrix of Standardised Data (Correlation Matrix)\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>1.0000000</td><td>0.7984717</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>0.7984717</td><td>1.0000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X1 & X2\\\\\n",
       "\\hline\n",
       "\tX1 & 1.0000000 & 0.7984717\\\\\n",
       "\tX2 & 0.7984717 & 1.0000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1 | X2 |\n",
       "|---|---|---|\n",
       "| X1 | 1.0000000 | 0.7984717 |\n",
       "| X2 | 0.7984717 | 1.0000000 |\n",
       "\n"
      ],
      "text/plain": [
       "   X1        X2       \n",
       "X1 1.0000000 0.7984717\n",
       "X2 0.7984717 1.0000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Principal Component Loadings (standardised data):\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>0.7071068 </td><td>-0.7071068</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>0.7071068 </td><td> 0.7071068</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tX1 & 0.7071068  & -0.7071068\\\\\n",
       "\tX2 & 0.7071068  &  0.7071068\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| X1 | 0.7071068  | -0.7071068 |\n",
       "| X2 | 0.7071068  |  0.7071068 |\n",
       "\n"
      ],
      "text/plain": [
       "   PC1       PC2       \n",
       "X1 0.7071068 -0.7071068\n",
       "X2 0.7071068  0.7071068"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Importance of Principal Components (standardised data):\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Var (Eigenval.)</th><td>1.7985</td><td>0.2015</td></tr>\n",
       "\t<tr><th scope=row>PVE</th><td>0.8992</td><td>0.1008</td></tr>\n",
       "\t<tr><th scope=row>cPVE</th><td>0.8992</td><td>1.0000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tVar (Eigenval.) & 1.7985 & 0.2015\\\\\n",
       "\tPVE & 0.8992 & 0.1008\\\\\n",
       "\tcPVE & 0.8992 & 1.0000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| Var (Eigenval.) | 1.7985 | 0.2015 |\n",
       "| PVE | 0.8992 | 0.1008 |\n",
       "| cPVE | 0.8992 | 1.0000 |\n",
       "\n"
      ],
      "text/plain": [
       "                PC1    PC2   \n",
       "Var (Eigenval.) 1.7985 0.2015\n",
       "PVE             0.8992 0.1008\n",
       "cPVE            0.8992 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAElBMVEUAAAAA/wAzMzNNTU3r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\nB/P3zS0f2W+vAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Sample Covariance Matrix:\")\n",
    "cov(df_demeaned)\n",
    "\n",
    "print(\"Covariance Matrix of Standardised Data (Correlation Matrix)\")\n",
    "# Standardisation alters original Cov Matrix\n",
    "cov(df_std)\n",
    "\n",
    "print(\"Principal Component Loadings (standardised data):\")\n",
    "pca_std$vectors\n",
    "\n",
    "print(\"Importance of Principal Components (standardised data):\")\n",
    "pca_std$importance\n",
    "\n",
    "ggplot(data = pca_std$df_var, aes(x=PC, y=var)) +\n",
    "    geom_bar(stat=\"identity\", fill=\"green\") + ylab(\"Variance (Eigenvalue)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this point of standardisation changing the obtained factor loadings more clearly, consider the bivariate case with uncorrelated variables and drastically differing variances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"****************************\"\n",
      "[1] \"*** Demeaned Data **********\"\n",
      "[1] \"****************************\"\n",
      "[1] \"Sample Covariance Matrix:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>11.43334373</td><td>-0.07325336</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>-0.07325336</td><td> 1.86232795</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X1 & X2\\\\\n",
       "\\hline\n",
       "\tX1 & 11.43334373 & -0.07325336\\\\\n",
       "\tX2 & -0.07325336 &  1.86232795\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1 | X2 |\n",
       "|---|---|---|\n",
       "| X1 | 11.43334373 | -0.07325336 |\n",
       "| X2 | -0.07325336 |  1.86232795 |\n",
       "\n"
      ],
      "text/plain": [
       "   X1          X2         \n",
       "X1 11.43334373 -0.07325336\n",
       "X2 -0.07325336  1.86232795"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Principal Components:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>-0.999970715</td><td>-0.007652994</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td> 0.007652994</td><td>-0.999970715</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tX1 & -0.999970715 & -0.007652994\\\\\n",
       "\tX2 &  0.007652994 & -0.999970715\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| X1 | -0.999970715 | -0.007652994 |\n",
       "| X2 |  0.007652994 | -0.999970715 |\n",
       "\n"
      ],
      "text/plain": [
       "   PC1          PC2         \n",
       "X1 -0.999970715 -0.007652994\n",
       "X2  0.007652994 -0.999970715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Var (Eigenval.)</th><td>11.4339</td><td>1.8618 </td></tr>\n",
       "\t<tr><th scope=row>PVE</th><td> 0.8600</td><td>0.1400 </td></tr>\n",
       "\t<tr><th scope=row>cPVE</th><td> 0.8600</td><td>1.0000 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tVar (Eigenval.) & 11.4339 & 1.8618 \\\\\n",
       "\tPVE &  0.8600 & 0.1400 \\\\\n",
       "\tcPVE &  0.8600 & 1.0000 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| Var (Eigenval.) | 11.4339 | 1.8618  |\n",
       "| PVE |  0.8600 | 0.1400  |\n",
       "| cPVE |  0.8600 | 1.0000  |\n",
       "\n"
      ],
      "text/plain": [
       "                PC1     PC2   \n",
       "Var (Eigenval.) 11.4339 1.8618\n",
       "PVE              0.8600 0.1400\n",
       "cPVE             0.8600 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"****************************\"\n",
      "[1] \"*** Standardised Data ******\"\n",
      "[1] \"****************************\"\n",
      "[1] \"Covariance Matrix:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td> 1.00000000</td><td>-0.01587497</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>-0.01587497</td><td> 1.00000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X1 & X2\\\\\n",
       "\\hline\n",
       "\tX1 &  1.00000000 & -0.01587497\\\\\n",
       "\tX2 & -0.01587497 &  1.00000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1 | X2 |\n",
       "|---|---|---|\n",
       "| X1 |  1.00000000 | -0.01587497 |\n",
       "| X2 | -0.01587497 |  1.00000000 |\n",
       "\n"
      ],
      "text/plain": [
       "   X1          X2         \n",
       "X1  1.00000000 -0.01587497\n",
       "X2 -0.01587497  1.00000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Principal Components:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>-0.7071068</td><td>-0.7071068</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td> 0.7071068</td><td>-0.7071068</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tX1 & -0.7071068 & -0.7071068\\\\\n",
       "\tX2 &  0.7071068 & -0.7071068\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| X1 | -0.7071068 | -0.7071068 |\n",
       "| X2 |  0.7071068 | -0.7071068 |\n",
       "\n"
      ],
      "text/plain": [
       "   PC1        PC2       \n",
       "X1 -0.7071068 -0.7071068\n",
       "X2  0.7071068 -0.7071068"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Var (Eigenval.)</th><td>1.0159</td><td>0.9841</td></tr>\n",
       "\t<tr><th scope=row>PVE</th><td>0.5079</td><td>0.4921</td></tr>\n",
       "\t<tr><th scope=row>cPVE</th><td>0.5079</td><td>1.0000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tVar (Eigenval.) & 1.0159 & 0.9841\\\\\n",
       "\tPVE & 0.5079 & 0.4921\\\\\n",
       "\tcPVE & 0.5079 & 1.0000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| Var (Eigenval.) | 1.0159 | 0.9841 |\n",
       "| PVE | 0.5079 | 0.4921 |\n",
       "| cPVE | 0.5079 | 1.0000 |\n",
       "\n"
      ],
      "text/plain": [
       "                PC1    PC2   \n",
       "Var (Eigenval.) 1.0159 0.9841\n",
       "PVE             0.5079 0.4921\n",
       "cPVE            0.5079 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAGFBMVEUAAAAAAP8zMzNNTU3r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\n3G3S5j0h9wAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "N <- 200\n",
    "sigma <- matrix(c(12, 0, 0, 2), byrow = T, nrow = 2)\n",
    "mu <- c(5, 15)\n",
    "\n",
    "X <- rmvnorm(N, mean = mu, sigma = sigma)\n",
    "colnames(X) <- c(\"X1\", \"X2\")\n",
    "X_demeaned <- scale(X, center=T, scale = F)\n",
    "\n",
    "df_demeaned <- as.data.frame(X_demeaned)\n",
    "\n",
    "print(\"****************************\")\n",
    "print(\"*** Demeaned Data **********\")\n",
    "print(\"****************************\")\n",
    "print(\"Sample Covariance Matrix:\")\n",
    "cov(df_demeaned)\n",
    "print(\"Principal Components:\")\n",
    "pca <- prco(X_demeaned)\n",
    "pca$vectors\n",
    "pca$importance\n",
    "\n",
    "p1 <-   ggplot(df_demeaned, aes(X1, X2)) + \n",
    "        xlim(-10, 10) + ylim(-5, 5) +\n",
    "        geom_point() + \n",
    "        geom_abline(aes(slope = pca$vectors[2, 1] / pca$vectors[1, 1], intercept = 0, colour=\"PC1\")) + \n",
    "        geom_abline(aes(slope = pca$vectors[2, 2] / pca$vectors[1, 2], intercept = 0, colour=\"PC2\")) +\n",
    "        theme(plot.title = element_text(hjust = 0.5)) +\n",
    "        labs(color = \"\") + xlab(\"X1 (demeaned)\") + ylab(\"X2 (demeaned)\") +\n",
    "        scale_color_manual(values = c(\"PC1\" = \"red\", \"PC2\" = \"blue\")) +\n",
    "        ggtitle(\"PCA with demeaned data\") +\n",
    "        coord_fixed()\n",
    "\n",
    "print(\"****************************\")\n",
    "print(\"*** Standardised Data ******\")\n",
    "print(\"****************************\")\n",
    "X_std <- scale(X, center = T, scale = T)\n",
    "df_std <- as.data.frame(X_std)\n",
    "pca_std <- prco(X_std)\n",
    "\n",
    "print(\"Covariance Matrix:\")\n",
    "cov(df_std)\n",
    "print(\"Principal Components:\")\n",
    "pca_std$vectors\n",
    "pca_std$importance\n",
    "\n",
    "p2 <-   ggplot(df_std, aes(X1, X2)) + \n",
    "        geom_point() + \n",
    "        geom_abline(aes(slope = pca_std$vectors[2, 1] / pca_std$vectors[1, 1], intercept = 0, colour=\"PC1\")) + \n",
    "        geom_abline(aes(slope = pca_std$vectors[2, 2] / pca_std$vectors[1, 2], intercept = 0, colour=\"PC2\")) +\n",
    "        theme(plot.title = element_text(hjust = 0.5)) +\n",
    "        labs(color = \"\") + xlab(\"X1 (stand.)\") + ylab(\"X2 (stand.)\") +\n",
    "        scale_color_manual(values = c(\"PC1\" = \"red\", \"PC2\" = \"blue\")) +\n",
    "        ggtitle(\"PCA with standardised data\") +\n",
    "        coord_fixed()\n",
    "\n",
    "plot_grid(p1, p2, align = \"h\", nrow = 1, rel_heights = c(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As X1 is the variable that has a much larger variance than X2 and the sample covariance between them is almost zero, the first principal component computed based on the demeaned data loads almost exclusively on X1. In other word, the first principal component is in the direction of X1, hence the red line in the left panel is essentially a flat horizontal line. About 86\\% of the total variance is captured by it. In turn, the second principal component must be orthogonal which yields a vertical line. \n",
    "\n",
    "While the point cloud of the standardised data looks the same, the standardisation procedure takes out the difference in variance: the newly generated variables have both a variance of 1. Since the correlation between the variables is almost zero, it follows that the factor loadings of the first principal component obtained with standardised data assign almost equal weight to both of the standardised variables.\n",
    "\n",
    "Thus, in the case of the covariance matrix approach, variables with a high variance will dominate the first principal component and one should be aware of this issue. For instance, it would actually matter whether length is measured in miles or in kilometres as this has implications for the variance of the measurements. With explanatory variables that are measured on different scales, it is therefore crucial to standardise in order to achieve scale invariance. However, one might be inclined to always standardise the variables, even if they are measured on the same scale, just to be precautious. However, the differences in variance may in fact be relevant and those high-variance variables might be more important when predicting the outcome. Hence, it is necessary to study the role of PCA in a regression context rather than as a tool for exploratory data analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Regression (PCR)\n",
    "\n",
    "To demonstrate the general procedure, consider the following set-up. We always draw a test and a training data set of equal size $N$. The explanatory variables are obtained as draws from a multivariate normal distribution. As the data is in any case demeaned, the means hardly matter. By contrast, the choice of the underlying covariance matrix is crucial. To ensure randomness of the covariance matrix it is constructed as follows:\n",
    "\n",
    "First, we create a matrix $W$ that has dimensions $p \\times p$ and its elements are all drawn from a standard normal distribution. As this matrix is not of full rank and we aim to obtain a covariance matrix that is positive semi-definite and of full rank, further manipulation is necessary: $ \\Sigma = W W^T + D$ where $D$ is a random diagonal matrix with only positive entries on the diagonal, which is ensured by drawing the diagonal from the standard uniform distribution. \n",
    "\n",
    "In the following, we consider the case of $N=500$ and $P = 20$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_x <- function(N, mu) {\n",
    "  \n",
    "  p <- length(mu) # number of predictors\n",
    "  \n",
    "  W = replicate(p, rnorm(p))\n",
    "  sigma = W%*%t(W) + diag(runif(p),nrow=p)\n",
    "  \n",
    "  X_train <- rmvnorm(n=N, mean=mu, sigma=sigma)  # randomly draw training data set\n",
    "  X_test <- rmvnorm(n=N, mean=mu, sigma=sigma)  # randomly draw test data set\n",
    "  \n",
    "  return(list(X_train=X_train, X_test = X_test, sigma = sigma))   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_y <- function(intercept, effects, X_train, X_test, sigma_x, PC, random, eps_sd) {\n",
    "  p <- dim(sigma_x)[1]\n",
    "  N_train <- dim(X_train)[1]\n",
    "  N_test <- dim(X_test)[1]\n",
    "  \n",
    "  if (random == 1) {\n",
    "\n",
    "    beta_vec <- sample(effects, size= p, replace=T)\n",
    "    y_train <- rep(intercept, N_train) + X_train %*% beta_vec + rnorm(n=N_train, mean=0, sd=eps_sd)\n",
    "    y_test <- rep(intercept, N_test) + X_test %*% beta_vec + rnorm(n=N_test, mean=0, sd=eps_sd)\n",
    "  \n",
    "  } else {\n",
    "    stopifnot(length(effects) == length(PC))\n",
    "    V <- eigen(sigma_x)$vectors[,PC]\n",
    "    y_train <- rep(intercept, N_train) + X_train %*% V %*% effects + rnorm(n=N_train, mean=0, sd=eps_sd)\n",
    "    y_test <- rep(intercept, N_test) + X_test %*% V %*% effects + rnorm(n=N_test, mean=0, sd=eps_sd)\n",
    "  }\n",
    "  return(list(y_train = y_train, y_test = y_test)) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_cv <- function(X, Y, k) {\n",
    "  \n",
    "  stopifnot(nrow(Y) == nrow(X))\n",
    "  \n",
    "  data <- cbind(Y, X)\n",
    "  \n",
    "  fold_i <- sample(rep(1:k, length.out = nrow(X)))\n",
    "  \n",
    "  mse <- matrix(NA, nrow=ncol(X), ncol=k)\n",
    "  \n",
    "  for (x in 1:k) {\n",
    "    val_i <- which(fold_i == x)\n",
    "    train <- data[-val_i,]\n",
    "    val <- data[val_i,]\n",
    "    \n",
    "    pca <- prco(train[,-1])\n",
    "    \n",
    "    for (j in 1:ncol(X)) {\n",
    "      Z_train <- cbind(rep(1, nrow(train)),train[,-1] %*% pca$vectors[,1:j])\n",
    "      Z_val <- cbind(rep(1, nrow(val)),val[,-1] %*% pca$vectors[,1:j])\n",
    "      \n",
    "      beta_hat <- solve(t(Z_train)%*%Z_train) %*% t(Z_train) %*% train[,1]\n",
    "      mse[j, x] <- mean((Z_val %*% beta_hat - val[,1]) ** 2)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  mse_cv <- cbind(1:ncol(X), rowMeans(mse))\n",
    "  colnames(mse_cv) <- c(\"M\", \"CV-MSE\")\n",
    "  \n",
    "  M <- match(min(mse_cv[,\"CV-MSE\"]), mse_cv[,\"CV-MSE\"])\n",
    "  pca_full_train <- prco(X)\n",
    "  cv_vectors <- pca_full_train$vectors[,1:M]\n",
    "  \n",
    "  Z <- cbind(rep(1, nrow(X)),X %*% cv_vectors)\n",
    "  beta_hat <- solve(t(Z)%*%Z) %*% t(Z) %*% Y\n",
    "  mse_train <- mean((Z %*% beta_hat - Y) ** 2)\n",
    "  \n",
    "  importance <- pca_full_train$importance[,1:M]\n",
    "  \n",
    "  return(list(mse_cv = mse_cv, mse_train = mse_train, beta_hat = beta_hat, M=M, cv_vectors=cv_vectors, importance=importance))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_comp <- function(X, Y, M) {\n",
    "    pca <- prco(X)\n",
    "    Z <- cbind(rep(1, nrow(X)),X %*% pca$vectors[,1:M])\n",
    "    beta_hat <- solve(t(Z)%*%Z) %*% t(Z) %*% Y\n",
    "    mse_train <- mean((Z %*% beta_hat - Y) ** 2)\n",
    "    \n",
    "    importance <- pca$importance[,1:M]\n",
    "    \n",
    "    return(list(mse_train = mse_train, beta_hat = beta_hat, vectors=pca$vectors[,1:M], importance=importance))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N <- 500\n",
    "p <- 20\n",
    "set.seed(3)\n",
    "X <- gen_x(N = N, sample(10:20, p, replace = T))\n",
    "X_train_d <- scale(X$X_train, center = T, scale = F)\n",
    "X_test_d <- scale(X$X_test, center = T, scale = F)\n",
    "\n",
    "y  <- gen_y(intercept=2, effects=rep(2, 5), X_train = X$X_train, X_test = X$X_test, sigma_x=X$sigma, PC = c(1, 2, 3, 4, 5),random = 0, eps_sd=1) \n",
    "\n",
    "y_train <- y$y_train\n",
    "y_test <- y$y_test\n",
    "\n",
    "pca <- prco(X_train_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of \"unsupervised learning\", the number of principal components to include would be determined without regard to the outcome variable $Y$. Thus, the previously discussed ad-hoc approaches are considered. Consistently, they would suggest $M=10$, so to include the principal components up to number 10. This is the one where arguably an *elbow* is identifiable from the Scree Plot and where the cumulative PVE reaches 90\\%. Also, this would be the last component larger than $0.7 \\bar{\\lambda}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th><th scope=col>PC3</th><th scope=col>PC4</th><th scope=col>PC5</th><th scope=col>PC6</th><th scope=col>PC7</th><th scope=col>PC8</th><th scope=col>PC9</th><th scope=col>PC10</th><th scope=col>PC11</th><th scope=col>PC12</th><th scope=col>PC13</th><th scope=col>PC14</th><th scope=col>PC15</th><th scope=col>PC16</th><th scope=col>PC17</th><th scope=col>PC18</th><th scope=col>PC19</th><th scope=col>PC20</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Var (Eigenval.)</th><td>66.7512</td><td>60.5089</td><td>52.9347</td><td>44.2527</td><td>42.6050</td><td>36.2178</td><td>32.1428</td><td>20.4739</td><td>16.7488</td><td>16.0852</td><td>10.9944</td><td>9.2573 </td><td>5.5978 </td><td>4.5052 </td><td>3.6399 </td><td>2.6003 </td><td>1.6471 </td><td>0.8152 </td><td>0.7556 </td><td>0.4647 </td></tr>\n",
       "\t<tr><th scope=row>PVE</th><td> 0.1556</td><td> 0.1410</td><td> 0.1234</td><td> 0.1032</td><td> 0.0993</td><td> 0.0844</td><td> 0.0749</td><td> 0.0477</td><td> 0.0390</td><td> 0.0375</td><td> 0.0256</td><td>0.0216 </td><td>0.0130 </td><td>0.0105 </td><td>0.0085 </td><td>0.0061 </td><td>0.0038 </td><td>0.0019 </td><td>0.0018 </td><td>0.0011 </td></tr>\n",
       "\t<tr><th scope=row>cPVE</th><td> 0.1556</td><td> 0.2966</td><td> 0.4200</td><td> 0.5232</td><td> 0.6225</td><td> 0.7069</td><td> 0.7819</td><td> 0.8296</td><td> 0.8686</td><td> 0.9061</td><td> 0.9317</td><td>0.9533 </td><td>0.9664 </td><td>0.9769 </td><td>0.9854 </td><td>0.9914 </td><td>0.9953 </td><td>0.9972 </td><td>0.9989 </td><td>1.0000 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllll}\n",
       "  & PC1 & PC2 & PC3 & PC4 & PC5 & PC6 & PC7 & PC8 & PC9 & PC10 & PC11 & PC12 & PC13 & PC14 & PC15 & PC16 & PC17 & PC18 & PC19 & PC20\\\\\n",
       "\\hline\n",
       "\tVar (Eigenval.) & 66.7512 & 60.5089 & 52.9347 & 44.2527 & 42.6050 & 36.2178 & 32.1428 & 20.4739 & 16.7488 & 16.0852 & 10.9944 & 9.2573  & 5.5978  & 4.5052  & 3.6399  & 2.6003  & 1.6471  & 0.8152  & 0.7556  & 0.4647 \\\\\n",
       "\tPVE &  0.1556 &  0.1410 &  0.1234 &  0.1032 &  0.0993 &  0.0844 &  0.0749 &  0.0477 &  0.0390 &  0.0375 &  0.0256 & 0.0216  & 0.0130  & 0.0105  & 0.0085  & 0.0061  & 0.0038  & 0.0019  & 0.0018  & 0.0011 \\\\\n",
       "\tcPVE &  0.1556 &  0.2966 &  0.4200 &  0.5232 &  0.6225 &  0.7069 &  0.7819 &  0.8296 &  0.8686 &  0.9061 &  0.9317 & 0.9533  & 0.9664  & 0.9769  & 0.9854  & 0.9914  & 0.9953  & 0.9972  & 0.9989  & 1.0000 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 | PC3 | PC4 | PC5 | PC6 | PC7 | PC8 | PC9 | PC10 | PC11 | PC12 | PC13 | PC14 | PC15 | PC16 | PC17 | PC18 | PC19 | PC20 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Var (Eigenval.) | 66.7512 | 60.5089 | 52.9347 | 44.2527 | 42.6050 | 36.2178 | 32.1428 | 20.4739 | 16.7488 | 16.0852 | 10.9944 | 9.2573  | 5.5978  | 4.5052  | 3.6399  | 2.6003  | 1.6471  | 0.8152  | 0.7556  | 0.4647  |\n",
       "| PVE |  0.1556 |  0.1410 |  0.1234 |  0.1032 |  0.0993 |  0.0844 |  0.0749 |  0.0477 |  0.0390 |  0.0375 |  0.0256 | 0.0216  | 0.0130  | 0.0105  | 0.0085  | 0.0061  | 0.0038  | 0.0019  | 0.0018  | 0.0011  |\n",
       "| cPVE |  0.1556 |  0.2966 |  0.4200 |  0.5232 |  0.6225 |  0.7069 |  0.7819 |  0.8296 |  0.8686 |  0.9061 |  0.9317 | 0.9533  | 0.9664  | 0.9769  | 0.9854  | 0.9914  | 0.9953  | 0.9972  | 0.9989  | 1.0000  |\n",
       "\n"
      ],
      "text/plain": [
       "                PC1     PC2     PC3     PC4     PC5     PC6     PC7     PC8    \n",
       "Var (Eigenval.) 66.7512 60.5089 52.9347 44.2527 42.6050 36.2178 32.1428 20.4739\n",
       "PVE              0.1556  0.1410  0.1234  0.1032  0.0993  0.0844  0.0749  0.0477\n",
       "cPVE             0.1556  0.2966  0.4200  0.5232  0.6225  0.7069  0.7819  0.8296\n",
       "                PC9     PC10    PC11    PC12   PC13   PC14   PC15   PC16  \n",
       "Var (Eigenval.) 16.7488 16.0852 10.9944 9.2573 5.5978 4.5052 3.6399 2.6003\n",
       "PVE              0.0390  0.0375  0.0256 0.0216 0.0130 0.0105 0.0085 0.0061\n",
       "cPVE             0.8686  0.9061  0.9317 0.9533 0.9664 0.9769 0.9854 0.9914\n",
       "                PC17   PC18   PC19   PC20  \n",
       "Var (Eigenval.) 1.6471 0.8152 0.7556 0.4647\n",
       "PVE             0.0038 0.0019 0.0018 0.0011\n",
       "cPVE            0.9953 0.9972 0.9989 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Mean Eigenvalue (Variance)\"\n",
      "[1] 21.44993\n",
      "[1] \"0.7 * (Mean Eigenvalue)\"\n",
      "[1] 15.01495\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAElBMVEUAAAAzMzNNTU3r6+v/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\nRCQAQhCRAAhBRAIgBBEJgBBEJP4DiKcmB4ZXwSoAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PVE <- pca$importance[\"PVE\",]\n",
    "cPVE <- pca$importance[\"cPVE\",]\n",
    "PC <- 1:p\n",
    "\n",
    "p1 <- ggplot(data.frame(PC, PVE), aes(x = PC, y = PVE)) + \n",
    "      theme(plot.title = element_text(hjust = 0.5)) +\n",
    "      geom_point() + geom_line() + ylab(\"Proportion of Variance Explained (PVE)\") +\n",
    "      geom_vline(xintercept = 10, color=\"red\") +\n",
    "      ggtitle(\"Scree Plot\") + scale_x_continuous(breaks = 1:p)\n",
    "\n",
    "p2 <- ggplot(data.frame(PC, cPVE), aes(x = PC, y = cPVE)) + \n",
    "      theme(plot.title = element_text(hjust = 0.5)) +\n",
    "      geom_point() + geom_line() + ylab(\"cum. Prop. of Variance Explained\") +\n",
    "      geom_vline(xintercept = 10, color=\"red\") +\n",
    "      ggtitle(\"Cumulative Prop. of Variance Explained\") + scale_x_continuous(breaks = 1:p) + scale_y_continuous(breaks = seq(0,1, by = 0.2))\n",
    "\n",
    "plot_grid(p1, p2, align = \"h\", nrow = 1, rel_heights = c(1, 1))\n",
    "\n",
    "pca$importance\n",
    "print(\"Mean Eigenvalue (Variance)\")\n",
    "print(mean(pca$values))\n",
    "print(\"0.7 * (Mean Eigenvalue)\")\n",
    "print(0.7 * mean(pca$values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, note that these approaches select $M$ without any consideration for $Y$ and only based on the assumption, that the most relevant statistical information is captured by the first couple of principal components which encapsulate a reasonably large farction of the roiginal variability. \n",
    "\n",
    "The benefit of simulated data is now that the the relationship of the explanatory variables with $Y$ can directly be controlled. Threfore, consider the following way of generating $Y$. Suppose we relate $Y$ to the demeaned (or standardised) variables $\\tilde{X}$:\n",
    "\n",
    "$$ Y = \\tilde{\\pmb X} \\beta + \\epsilon \\text{ ,}$$\n",
    "\n",
    "note that the statistical inference is not affected by demeaning or standardisation the explanatory variables as the estimates are simply adjusted accrodingly. As the matrix of all principal components (eigenvectors), $\\pmb \\phi$, is an orthogonal matrix which implies that the inverse of it is identical to its transposed, so $ \\pmb \\phi^T = \\pmb \\phi ^{-1}$, hence: $\\pmb \\phi \\pmb \\phi^T = \\pmb I = \\pmb \\phi^T \\pmb \\phi$. Thus, the structural equation can be re-written:\n",
    "\n",
    "$$ Y = \\tilde{\\pmb X} \\pmb \\phi \\pmb \\phi^T + \\epsilon$$\n",
    "\n",
    "and substituting $ \\pmb Z = \\tilde{\\pmb X} \\pmb \\phi $ yields\n",
    "\n",
    "$$ Y = \\pmb Z \\theta + \\epsilon \\text{ ,} $$\n",
    "\n",
    "where $\\theta = \\pmb \\phi^T \\beta$. Now assume, that $\\beta$ is only in the direction of the $j$-th eigenvector $\\phi_j$, which is $\\phi_j = \\alpha \\beta$ with the scalar $\\alpha \\neq 0$. Hence, $\\theta_j = \\phi_j^T \\beta = \\alpha \\beta^T \\beta$ and $\\theta_{j \\, '} = \\phi_{j \\, '}^T \\beta = 0$ for $j \\neq j \\, '$. Thus, the outcome is fully determined by the principal component $Z_j$:\n",
    "\n",
    "$$ Y = \\theta_j Z_j + \\epsilon = \\theta_j \\tilde{\\pmb X} \\phi_j + \\epsilon \\text{ .}$$\n",
    "\n",
    "The other principal components do not affect the outcome at all as their coefficients are set to zero. This reasoning can be extended to allow the outcome $Y$ to be determined by two or more principal components.\n",
    "\n",
    "For the simulation this implies the following. Based on the \"true\" underlying covariance matrix $\\Sigma$, which is randomly generated, the \"true\" eigenvectors can be extracted. However, it is not necessarily the case that these \"true\" eigenvectors also match the empirically obtained principal components loadings as the random sampling of the explanatory variables from a multivariate normal distribution induces *noise*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of Principal Components chosen by CV:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "15"
      ],
      "text/latex": [
       "15"
      ],
      "text/markdown": [
       "15"
      ],
      "text/plain": [
       "[1] 15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAFVBMVEUAAAAAAP8zMzNNTU3r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\nAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(12)\n",
    "pcr <- pcr_cv(X=X_train_d, Y=y_train, k=10)\n",
    "\n",
    "print(\"Number of Principal Components chosen by CV:\")\n",
    "pcr$M\n",
    "data <- data.frame(pcr$mse_cv)\n",
    "\n",
    "ggplot(data, aes(x = M, y = CV.MSE)) + \n",
    "      theme(plot.title = element_text(hjust = 0.5)) +\n",
    "      geom_point() + geom_line() + ylab(\"CV-MSE\") + geom_vline(xintercept = pcr$M, color=\"red\") +\n",
    "      geom_vline(xintercept = 5, color=\"blue\", linetype=\"dashed\") +\n",
    "      ggtitle(\"Cross-Validation MSE per number of included PCs\") + scale_x_continuous(breaks = 1:20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimal CV-MSE based on 10-fold CV is obtained for $M=15$ and thus way more principal components are included as justified based on the true data generating process, where the outcome $Y$ was related only to the first 5 eigenvectors of the \"true\" covariance matrix. This is also larger as the number of principal components suggested by the ad-hoc approaches - but as already discussed, they are dervied without any regard for the outcome variable. It has to be emphasized that taking the minimum criterion with respect to the CV-MSE leaves room for some random interferences. As can be seen from the plot, after the 6th PC the CV-MSE is essentially flat, i.e. the additional principal components included do not improve the fit. However, just by chance it may occur that an additional PC picks up on some random noise which results in a *slightly* better performance. This explains why taking the absolute minimum based on CV may not be reasonable. Instead, it is very much suggested to plot the CV-MSE and identify the appropriate number of principal components to include using eyeballing. In this case, it would probably lead to including 7 or 8 PCs rather than 15, achieving a larger degree of dimensionality reduction.\n",
    "\n",
    "But why is the CV-MSE still distinctly higher with 5 principal compnents than with 6, although the data generating process of $Y$ is based on 5 eigenvectors? While also the CV-procedure also induces some randomness when randomly splitting the training sample into $k$ folds, this can rather be explained by the noise introduced in the sampling process when randomly drawing the explanatory variables from the multivariate normal distribution, as already pointed to above. To illustrate this, we repeat the same procedure with the only modification of choosing $N = 50.000$ instead of $N = 500$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of Principal Components chosen by CV:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "18"
      ],
      "text/latex": [
       "18"
      ],
      "text/markdown": [
       "18"
      ],
      "text/plain": [
       "[1] 18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>M</th><td>  1.0000</td><td>  2.0000</td><td>  3.0000</td><td>  4.0000</td><td>5.0000  </td><td>6.0000  </td><td>7.0000  </td><td>8.0000  </td><td>9.0000  </td><td>10.0000 </td><td>11.000  </td><td>12.0000 </td><td>13.000  </td><td>14.0000 </td><td>15.0000 </td><td>16.0000 </td><td>17.0000 </td><td>18.0000 </td><td>19.0000 </td><td>20.0000 </td></tr>\n",
       "\t<tr><th scope=row>CV-MSE</th><td>756.5056</td><td>602.9194</td><td>347.9348</td><td>144.8953</td><td>4.1426  </td><td>1.0329  </td><td>1.0142  </td><td>1.0093  </td><td>1.0051  </td><td> 1.0041 </td><td> 1.003  </td><td> 1.0021 </td><td> 1.002  </td><td> 1.0018 </td><td> 1.0018 </td><td> 1.0015 </td><td> 1.0015 </td><td> 1.0015 </td><td> 1.0016 </td><td> 1.0016 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllll}\n",
       "\tM &   1.0000 &   2.0000 &   3.0000 &   4.0000 & 5.0000   & 6.0000   & 7.0000   & 8.0000   & 9.0000   & 10.0000  & 11.000   & 12.0000  & 13.000   & 14.0000  & 15.0000  & 16.0000  & 17.0000  & 18.0000  & 19.0000  & 20.0000 \\\\\n",
       "\tCV-MSE & 756.5056 & 602.9194 & 347.9348 & 144.8953 & 4.1426   & 1.0329   & 1.0142   & 1.0093   & 1.0051   &  1.0041  &  1.003   &  1.0021  &  1.002   &  1.0018  &  1.0018  &  1.0015  &  1.0015  &  1.0015  &  1.0016  &  1.0016 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| M |   1.0000 |   2.0000 |   3.0000 |   4.0000 | 5.0000   | 6.0000   | 7.0000   | 8.0000   | 9.0000   | 10.0000  | 11.000   | 12.0000  | 13.000   | 14.0000  | 15.0000  | 16.0000  | 17.0000  | 18.0000  | 19.0000  | 20.0000  |\n",
       "| CV-MSE | 756.5056 | 602.9194 | 347.9348 | 144.8953 | 4.1426   | 1.0329   | 1.0142   | 1.0093   | 1.0051   |  1.0041  |  1.003   |  1.0021  |  1.002   |  1.0018  |  1.0018  |  1.0015  |  1.0015  |  1.0015  |  1.0016  |  1.0016  |\n",
       "\n"
      ],
      "text/plain": [
       "       [,1]     [,2]     [,3]     [,4]     [,5]   [,6]   [,7]   [,8]   [,9]  \n",
       "M        1.0000   2.0000   3.0000   4.0000 5.0000 6.0000 7.0000 8.0000 9.0000\n",
       "CV-MSE 756.5056 602.9194 347.9348 144.8953 4.1426 1.0329 1.0142 1.0093 1.0051\n",
       "       [,10]   [,11]  [,12]   [,13]  [,14]   [,15]   [,16]   [,17]   [,18]  \n",
       "M      10.0000 11.000 12.0000 13.000 14.0000 15.0000 16.0000 17.0000 18.0000\n",
       "CV-MSE  1.0041  1.003  1.0021  1.002  1.0018  1.0018  1.0015  1.0015  1.0015\n",
       "       [,19]   [,20]  \n",
       "M      19.0000 20.0000\n",
       "CV-MSE  1.0016  1.0016"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAFVBMVEUAAAAAAP8zMzNNTU3r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\nwsiGnQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_l <- 50000\n",
    "set.seed(3)\n",
    "X_l <- gen_x(N = N_l, sample(10:20, p, replace = T))\n",
    "X_l_train_d <- scale(X_l$X_train, center = T, scale = F)\n",
    "X_l_test_d <- scale(X_l$X_test, center = T, scale = F)\n",
    "\n",
    "y_l  <- gen_y(intercept=2, effects=rep(2, 5), X_train = X_l$X_train, X_test = X_l$X_test, sigma_x=X_l$sigma, PC = c(1, 2, 3, 4, 5),random = 0, eps_sd=1) \n",
    "y_l_train <- y_l$y_train\n",
    "\n",
    "pcr_l <- pcr_cv(X=X_l_train_d, Y=y_l_train, k=10)\n",
    "\n",
    "print(\"Number of Principal Components chosen by CV:\")\n",
    "pcr_l$M\n",
    "data <- data.frame(pcr_l$mse_cv)\n",
    "round(t(pcr_l$mse_cv),4)\n",
    "\n",
    "ggplot(data, aes(x = M, y = CV.MSE)) + \n",
    "      theme(plot.title = element_text(hjust = 0.5)) +\n",
    "      geom_point() + geom_line() + ylab(\"CV-MSE\") + geom_vline(xintercept = pcr_l$M, color=\"red\") +\n",
    "      geom_vline(xintercept = 5, color=\"blue\", linetype=\"dashed\") +\n",
    "      ggtitle(\"Cross-Validation MSE per number of included PCs\") + scale_x_continuous(breaks = 1:20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, the CV-MSE starts becoming a flat horizontal line when the \"true\" number of principal components is reached at $M = 5$. With such sample sizes, eyeballing the CV-MSE might be more likely to identify the \"appropriate\" number of principal components to include. The minimum criterion, however, would probably not ensure this: in this case, $M=18$ would be chosen. But even in this case, the CV-MSE with 6 principal components is distincly smaller than the CV-MSE with 5 principal components. With this sample size, the situation improves, but still, the empirically obtained principal components do not exactly resemble the \"true\" eigenvectors used when generating $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the case with $N = 500$, it is now time to compare the performance on the held-out test data set. Therefore, we compare the Test-MSE of OLS to the one obtained with CV (i.e. with 15 principal components). Additionally, the Test-MSEs are displayed when including up to the 20th principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Test-MSE OLS</th><th scope=col>Test-MSE PCR/CV</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1.0496</td><td>3.4815</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{ll}\n",
       " Test-MSE OLS & Test-MSE PCR/CV\\\\\n",
       "\\hline\n",
       "\t 1.0496 & 3.4815\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Test-MSE OLS | Test-MSE PCR/CV |\n",
       "|---|---|\n",
       "| 1.0496 | 3.4815 |\n",
       "\n"
      ],
      "text/plain": [
       "     Test-MSE OLS Test-MSE PCR/CV\n",
       "[1,] 1.0496       3.4815         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>M</th><td>  1.0000</td><td>  2.0000</td><td>  3.0000</td><td>  4.0000</td><td>  5.0000</td><td> 6.0000 </td><td>7.0000  </td><td>8.0000  </td><td>9.000   </td><td>10.000  </td><td>11.0000 </td><td>12.0000 </td><td>13.0000 </td><td>14.0000 </td><td>15.0000 </td><td>16.0000 </td><td>17.0000 </td><td>18.0000 </td><td>19.0000 </td><td>20.0000 </td></tr>\n",
       "\t<tr><th scope=row>Test-MSE</th><td>696.1666</td><td>540.7817</td><td>411.3105</td><td>225.6567</td><td>225.6705</td><td>26.0683 </td><td>8.1784  </td><td>4.4375  </td><td>4.438   </td><td> 4.049  </td><td> 3.5874 </td><td> 3.5134 </td><td> 3.5129 </td><td> 3.5117 </td><td> 3.4815 </td><td> 3.4856 </td><td> 3.4801 </td><td> 3.4804 </td><td> 3.4784 </td><td> 3.4742 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllll}\n",
       "\tM &   1.0000 &   2.0000 &   3.0000 &   4.0000 &   5.0000 &  6.0000  & 7.0000   & 8.0000   & 9.000    & 10.000   & 11.0000  & 12.0000  & 13.0000  & 14.0000  & 15.0000  & 16.0000  & 17.0000  & 18.0000  & 19.0000  & 20.0000 \\\\\n",
       "\tTest-MSE & 696.1666 & 540.7817 & 411.3105 & 225.6567 & 225.6705 & 26.0683  & 8.1784   & 4.4375   & 4.438    &  4.049   &  3.5874  &  3.5134  &  3.5129  &  3.5117  &  3.4815  &  3.4856  &  3.4801  &  3.4804  &  3.4784  &  3.4742 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| M |   1.0000 |   2.0000 |   3.0000 |   4.0000 |   5.0000 |  6.0000  | 7.0000   | 8.0000   | 9.000    | 10.000   | 11.0000  | 12.0000  | 13.0000  | 14.0000  | 15.0000  | 16.0000  | 17.0000  | 18.0000  | 19.0000  | 20.0000  |\n",
       "| Test-MSE | 696.1666 | 540.7817 | 411.3105 | 225.6567 | 225.6705 | 26.0683  | 8.1784   | 4.4375   | 4.438    |  4.049   |  3.5874  |  3.5134  |  3.5129  |  3.5117  |  3.4815  |  3.4856  |  3.4801  |  3.4804  |  3.4784  |  3.4742  |\n",
       "\n"
      ],
      "text/plain": [
       "         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]    [,7]   [,8]  \n",
       "M          1.0000   2.0000   3.0000   4.0000   5.0000  6.0000 7.0000 8.0000\n",
       "Test-MSE 696.1666 540.7817 411.3105 225.6567 225.6705 26.0683 8.1784 4.4375\n",
       "         [,9]  [,10]  [,11]   [,12]   [,13]   [,14]   [,15]   [,16]   [,17]  \n",
       "M        9.000 10.000 11.0000 12.0000 13.0000 14.0000 15.0000 16.0000 17.0000\n",
       "Test-MSE 4.438  4.049  3.5874  3.5134  3.5129  3.5117  3.4815  3.4856  3.4801\n",
       "         [,18]   [,19]   [,20]  \n",
       "M        18.0000 19.0000 20.0000\n",
       "Test-MSE  3.4804  3.4784  3.4742"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Training-MSE OLS:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "1.03529248008105"
      ],
      "text/latex": [
       "1.03529248008105"
      ],
      "text/markdown": [
       "1.03529248008105"
      ],
      "text/plain": [
       "[1] 1.035292"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Training-MSE with M=20:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "1.03529248008105"
      ],
      "text/latex": [
       "1.03529248008105"
      ],
      "text/markdown": [
       "1.03529248008105"
      ],
      "text/plain": [
       "[1] 1.035292"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OLS\n",
    "X_t <- cbind(rep(1, nrow(X$X_train)), X$X_train)\n",
    "beta_ols <- solve(t(X_t) %*% X_t) %*% t(X_t) %*% y_train\n",
    "mse_train_ols <- mean((cbind(rep(1, nrow(X$X_train)), X$X_train) %*% beta_ols - y_train)**2)\n",
    "mse_test_ols <- mean((cbind(rep(1, nrow(X$X_test)), X$X_test) %*% beta_ols - y_test)**2)\n",
    "\n",
    "# PCR with 15 PCs (CV solution)\n",
    "Z <- cbind(rep(1, nrow(X$X_test)),X_test_d  %*% pcr$cv_vectors)\n",
    "mse_test_pcrCV <- mean((Z %*% pcr$beta_hat - y_test) ** 2)\n",
    "\n",
    "res1 <- matrix(c(mse_test_ols, mse_test_pcrCV), nrow = 1)\n",
    "colnames(res1) <- c(\"Test-MSE OLS\", \"Test-MSE PCR/CV\")\n",
    "\n",
    "# Test MSEs for M= 1 , ..., 20\n",
    "mse_m <- matrix(NA, ncol=20, nrow=2)\n",
    "for (m in 1:20) {\n",
    "    pcr_m <- pcr_comp(X = X_train_d, Y=y_train, M=m)\n",
    "    Z <- cbind(rep(1, nrow(X$X_test)),X_test_d  %*% pcr_m$vectors)\n",
    "    mse_m[1, m] <- m\n",
    "    mse_m[2, m] <- mean((Z %*% pcr_m$beta_hat - y_test) ** 2) \n",
    "}\n",
    "\n",
    "rownames(mse_m) <- c(\"M\", \"Test-MSE\")\n",
    "\n",
    "round(res1, 4)\n",
    "round(mse_m, 4)\n",
    "\n",
    "print(\"Training-MSE OLS:\")\n",
    "mse_train_ols\n",
    "print(\"Training-MSE with M=20:\")\n",
    "pcr_comp(X = X_train_d, Y=y_train, M=20)$mse_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the OLS Test-MSE performs better than any Principal Component Regression. Notably, even PCR with $M=20$, i.e. no dimensionality reduction, yields a higher Test-MSE than OLS. This casts doubts about the reliability of the employed functions. However, both yield the same MSE when being applied to the training data, so there does not seem to be a mistake in the functions.\n",
    "\n",
    "This result could be due to the fact that $P << N$ anyway as $N=500$ and $P=20$, so *overfitting* is likely not to be an issue. Therefore, it is worth investigating this more thoroughly in a short simulating study featuring situations with different relations of $M$ to $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_func <- function(N, P, intercept, effects, PC, random_effects, eps_sd, k, M_true) {\n",
    "  X <- gen_x(N = N, sample(10:20, size=P, replace = T))\n",
    "  X_train_d <- scale(X$X_train, center = T, scale = F)\n",
    "  X_test_d <- scale(X$X_test, center = T, scale = F)\n",
    "  \n",
    "  y  <- gen_y(intercept=intercept, effects=effects, PC = PC,random = random_effects, eps_sd=eps_sd, X_train = X$X_train, X_test = X$X_test, sigma_x=X$sigma) \n",
    "  y_train <- y$y_train\n",
    "  y_test <- y$y_test\n",
    "  \n",
    "  # PCR / CV\n",
    "  pcr <- pcr_cv(X=X_train_d, Y=y_train, k=k)\n",
    "  Z <- cbind(rep(1, nrow(X$X_test)),X_test_d  %*% pcr$cv_vectors)\n",
    "  mse_test_pcr <- mean((Z %*% pcr$beta_hat - y_test) ** 2)\n",
    "\n",
    "  # OLS  \n",
    "  X_t <- cbind(rep(1, nrow(X$X_test)), X$X_train)\n",
    "  beta_ols <- solve(t(X_t) %*% X_t) %*% t(X_t) %*% y_train\n",
    "  mse_test_ols <- mean((cbind(rep(1, nrow(X$X_test)), X$X_test) %*% beta_ols - y_test) ** 2)\n",
    "  \n",
    "  return(list(mse_test_ols = mse_test_ols, mse_test_pcr = mse_test_pcr,  M = pcr$M))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in `[<-`(`*tmp*`, x, 1, value = n): Indizierung auerhalb der Grenzen\n",
     "output_type": "error",
     "traceback": [
      "Error in `[<-`(`*tmp*`, x, 1, value = n): Indizierung auerhalb der Grenzen\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "reps <- 30\n",
    "res <- matrix(NA, nrow=9, ncol=4)\n",
    "colnames(res) <- c(\"N\",\"MSE_OLS\", \"MSE_PCR\", \"selected M (CV)\")\n",
    "\n",
    "x <- 30\n",
    "for (n in seq(100, 500, by=50)) {\n",
    "  simulation <-\n",
    "    replicate(reps, sim_func(\n",
    "      N = n, \n",
    "      P = 75, \n",
    "      intercept= 0, \n",
    "      effects = sample(5:10, size=5, replace=T), \n",
    "      PC = 1:5, \n",
    "      random_effects = 0, \n",
    "      eps_sd = 1, \n",
    "      k = 5))\n",
    "  \n",
    "  sim <-  mapply(simulation, FUN = as.numeric)\n",
    "  sim <- matrix(data = sim, ncol = reps)\n",
    "  mean_values <- rowMeans(sim)\n",
    "  \n",
    "  res[x, 1] <- n\n",
    "  res[x, 2] <- mean_values[1]\n",
    "  res[x, 3] <- mean_values[2]\n",
    "  res[x, 4] <- mean_values[3]\n",
    "\n",
    "  x <- x + 1 \n",
    "}\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But even with $P=75$ and $N$ starting from 100, the OLS Test-MSE clearly dominates the Test-MSE of PCR with the number of components chosen by CV. As already discussed above, the CV approach based on the absolute minimum of the CV-MSE yields only a slight dimensionality reduction as still more than 70 principal components are retained on average over the 30 replications of this simulation study. So even in this situation deemed suitable for the PCR, it performs way worse than the OLS approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asset Index as Empirical Application\n",
    "\n",
    "So far, we discussed the motivation for using Principal Component Analysis, its properties and its usage in the regression context. This section aims to illustrate a way that PCA is currently utilised in economics, specifically in development economics. Essentially, PCA is used to construct a proxy variable of something that is either generally unoberservable or not just not elicited in a survey.\n",
    "\n",
    "The problem motivating the use of PCA can be described as follows. Development economists are often interested in the economic status of a household - for instance, in order to study the extent to which economic inequality prevails or in order to relate household welfare to another variable such as educational outcomes of children (e.g. McKenzie, 2005, or Filmer/Pritchett, 2001). It seems natural to use household income or consumption expenditure as measures of the economic status of a household. However, in the development context there are two issues that render this impossible in many cases. First, survey data collected in developing countries often goes along with reporting errors that render the usage of income or consumption data infeasible. Second, and even more fundamental, surveys conducted in developing countries commonly do not even contain questions with respect to those aspects. One example is the Demographic and Health Survey (DHS): conducted in a broad range of developing countries with a nationally-representative sample of households, these data are frequently used in development studies but do not generally contain information on household consumption expenditure or income.\n",
    "\n",
    "However, those survey data usually include information regarding an household's ownership of certain assets, such as a bicycle or a refrigerator, or with respect to housing standards, e.g. the materials used for housing construction. Therefore, the idea is to utilise those survey items in order to construct a proxy variable for household wealth. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Motivation:\n",
    "\n",
    "- survey data collected in developing countries often does not contain information on hh's income or consumption expenditures\n",
    "    - see Demographic and Health Survey (DHS): conducted in broad range of developing countries, \"nationally-representative household surveys that provide data for a wide range of monitoring and impact evaluation indicators in the areas of population, health, and nutrition\", frequently used in development studies\n",
    "    - generally no information on income or expenditures, therefore a wealth index is constructed as \"a composite measure of a household's cumulative living standard\", \"using easy-to-collect data on a household's ownership of selected assets, such as televisions and bicycles; materials used for housing construction; and types of water access and sanitation facilities. \n",
    "    \n",
    "Aim:\n",
    "\n",
    "- Construct \"asset index\" to proxy for \"a household's long-run economic status\" (Filmer/Pritchett 2001)\n",
    "    - constructed usually from data on asset ownership or housing characteristics (e.g. Filmer/Pritchett 2001)\n",
    "    \n",
    "Choosing appropriate weights?\n",
    "\n",
    "- equal weights (simply sum over the variables in the case of Dummy variables) has the \"appeal of simplicity\" but this \"masks the fact that the imposition of numeric equality is completely arbitrary\" (Filmer/Pritchett 2001)\n",
    "- \"simply adding all asset variables separately in a linear regression would implicitly create weights\", but most assets may exert both an indirect (as a proxy of wealth) but also a direct effect, thus it is not possible to isolate the effect of an increase in wealth (Filmer/Pritchett 2001)\n",
    "\n",
    "Standardization of variables. Based on the index, build percentiles: bottom 40% poor, upper 20% rich\n",
    "\n",
    "Crucial assumption:\n",
    "\n",
    "- hh's \"long-run wealth explains the maximum variance (and covariance) in the asset variables\", \"no way to test this assumption directly\" (Filmer/Pritchett 2001)\n",
    "- \"internal/external coherence\" (Filmer/Pritchett 2001):\n",
    "    - internal coherence: \"poor\" should have lower mean of assets associated with high wealth and high means for assets associated with low wealth\n",
    "    - external coherence: comparison across Indian states using state-level poverty rates\n",
    "- compare with consumption expenditures, when both available in the survey (Filmer/Pritchett 2001)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- in case of dummy variables: change from 0 to 1\n",
    "\n",
    "Factor Analysis:\n",
    "\n",
    "- Filmer/Pritchett (2001): list FA as alternative approach and state that the ranks derived under both approaches has 0.988 Spearman rank correlation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taste1 <- c(rep(1, 10), rep(0, 10))\n",
    "wealth1 <- c(0, 0, rep(1, 8), 0, 0, rep(1, 8))\n",
    "sit1 <- cbind(wealth1, taste1)\n",
    "df1 <- as.data.frame(sit1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "ggplot(df1, aes(wealth1, taste1)) +\n",
    "  geom_point(position = position_jitter(w = 0.01, h = 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov(sit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prcomp(sit1, center=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_w <- runif(2000, min = 0, max = 1)\n",
    "w <- as.integer(draw_w > 4/20)\n",
    "mean(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_t <- runif(2000, min=0, max=1)\n",
    "t <- as.integer(draw_t > 0.5)\n",
    "mean(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- cbind(w, t)\n",
    "cov(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prcomp(x, center=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
