{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dimensionality Reduction and Empirical Application\n",
    "\n",
    "### A Project by Marc Lipfert, Matriculation Number: 3220513\n",
    "\n",
    "### Computational Statistics (Summer Semester 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[1.) Introduction](#introduction)\n",
    "\n",
    "2.) Dimensionality Reduction Using Principal Component Analysis (PCA)\n",
    "\n",
    "3.) Outline of the Empirical Setting\n",
    "\n",
    "4.) Comparison of PCA and Factor Analysis\n",
    "\n",
    "5.) Simulation Study\n",
    "\n",
    "6.) Empirical Application to Actual Survey Data\n",
    "\n",
    "7.) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1.) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pca'></a>\n",
    "## 2.) Dimensionality Reduction Using Principal Component Analysis (PCA)\n",
    "\n",
    "This section aims to introduce Principal Component Analysis (PCA) and thereby intends to answer the following questions: In which contexts is PCA a useful tool? And what are its properties and how is it conducted? \n",
    "\n",
    "*(a) In which contexts is PCA a useful tool?*\n",
    "\n",
    "PCA is essentially a dimensionality reduction method. In the case of a data set with many variables (\"high dimensionality\"), an empirical model based on an extensive set of variables is usually difficult to interpret. Further, the predictive performance of such a model may be poor due to overfitting. Therefore, the aim is often to create a small(er) set of new variables which capture most of the statistical information contained in the original variables. PCA intends to achieve exactly this, where those newly generated variables are called *Principal Components* and they are linear combinations of the original variables. \n",
    "\n",
    "To illustrate in more detail what this means and why it is done, it is useful to fix ideas and draw on a formal framework. The set-up is intended to resemble a situation that is frequently encountered in microeconometrics. Assume that a survey is conducted among randomly selected individuals. The sample size is $N$ and the econometrician is interested in the relationship between the outcome $Y$ and a set of $P$ explanatory variables $X_1 , \\ldots , X_P$. The empirical model (structural equation) for an indivdidual $i$ is assumed to be as follows:\n",
    "\n",
    "$$ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\ldots + \\beta_P X_{Pi} + \\epsilon_i \\text{,} $$\n",
    "\n",
    "where $\\epsilon$ denotes an error term that is assumed to have a conditional expectation of zero: $E[\\epsilon | X] = 0$. Using matrix notation, $ \\pmb Y$ and $\\pmb \\epsilon$ denote column vectors each of length $N$ and $\\pmb X$ is a $N \\times (P + 1)$ matrix. Then, the re-written structural equation reads: \n",
    "\n",
    "$$ Y = \\pmb X \\beta + \\epsilon \\text{ , where } \\pmb X = \\begin{pmatrix} 1 & X_{11} & X_{21} & \\dots & X_{P1} \\\\\n",
    "1 & X_{12} & X_{22} & \\dots & X_{P2} \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & X_{1N} & X_{2N} & \\dots & X_{PN} \\end{pmatrix} $$. \n",
    "\n",
    "The task of the econometrician is to find estimates $\\hat{\\beta}$ of the population parameters $\\beta$, usually obtained by minimizing the sum of squared residuals (i.e. OLS).  \n",
    "\n",
    "There are two problems that go along with high dimensional data, i.e. large $P$. First, when $N$ is only slightly larger than $P$, *overfitting* is likely to occur. In such a setting with a large number of explanatory variables relative to the sample size, the estimated coefficients of the explanatory variables might pick up on random noise rather than capturing the (\"true\") structural parameters. This can be viewed as an instance of the bias-variance trade-off: given standard assumptions, the numerous estimated coefficients would be unbiased but the variance of the obtained predictions would be large and thus highly dependent on the random sample that the model is fit on. Put differently, the estimated function $\\hat{f}$ with coefficients $ \\hat{\\beta}$ would yield poor predictions (or respectively, high prediction errors) when being applied to a different random sample of the population (\"test data set\").\n",
    "\n",
    "The second issue in the context of high dimensional data is the limited interpretability. Given the large number of explantory variables, a meaningful interpretation of the model may no longer be feasible. One might be inclined to interpret the estimated coefficients separately - but since high dimensionality can go along large correlations among explanatory variables, the issue of multicollinearity may render this approach useless as it cannot be pinned down which variable is truely predictive of the outcome.\n",
    "\n",
    "Dimensionality reduction can be employed to address those two issues. Principal Component Analysis (PCA) is probably the most common method to achieve this. As already stated, it entails finding linear combinations of the explanatory variables that encapsulate as much of the statistical information (\"variability\") contained in the original variables as possible. The actual procedure is outlined in the following.\n",
    "\n",
    "*(b) What are the properties of PCA and how is it conducted?*\n",
    "\n",
    "To begin with, it is common practice to centre (or \"demean\") the explanatory variables, which is done as follows: \n",
    "\n",
    "$$ \\tilde{x}_{ij} = x_{ij} - \\bar{x}_j \\text{,}$$\n",
    "\n",
    "where $\\bar{x}_j$ denotes the sample mean of the $j$th explanatory variable across the $N$ individuals. Importantly, this does not alter the solution besides centring it at zero. Note that when the units of measurement differ across the explanatory variables, e.g. height in cm and temperature in degree Celsius, it is advisable to go even further and standardise the variables to mean zero and standard deviation of 1. This is because of an undesirable feature that PCA has which is the sensitivity of the results to the units of measurement. A change in the units of measurement of a variable also affects its variance - and as the principal components are constructed such that they capture the greatest possible variability of the original variables, it follows that the principal components would change. This issue of standardisation will be picked up on in the following section.\n",
    "\n",
    "We are then interested in obtaining linear combinations $Z_1, \\dots, Z_M$ of the centred variables, where $M \\leq P$. Those linear combinations are referred to as prinicpal components and they are ordered according to their variance in a descending fashion, thus the first principal component $Z_1$ is the linear combination of the centred explanatory variables,\n",
    "\n",
    "$$ Z_1 = \\phi_{11} \\tilde{X}_1 + \\phi_{12} \\tilde{X}_2 + \\dots + \\phi_{1P} \\tilde{X}_P = \\sum_{j=1}^P \\phi_{1j} \\tilde{X}_j = \\tilde{\\pmb X} \\phi_1 \\, \\text{ ,}$$\n",
    "\n",
    "that yields the highest variance. The scalars $\\phi_{11}, \\phi_{12}, \\dots, \\phi_{1P}$ are referred to as *loadings* of the first principal component and $\\phi_1$ denotes the corresponding column vector $\\phi_1 = (\\phi_{11} \\, \\phi_{12}  \\, \\dots \\,  \\phi_{1P})^T$. \n",
    "\n",
    "To derive the actual loadings of the first principal component, we have to note that any such linear combination has its variance given by $Var(\\tilde{\\pmb X} \\phi)  = \\phi^T \\pmb S \\phi$, where $\\pmb S$ denotes the sample covariance matrix of the (centred) explanatory variables. Thus, the vector $\\phi_1$ must be obtained, which maximises the quadratic form $\\phi^T \\pmb S \\phi$. To ensure a well-defined solution to this problem, $\\phi$ is required to be a unit-norm vector, i.e. $\\phi^T \\phi = 1$. Put together, the constrained maximisation problem can be written as \n",
    "\n",
    "$$\\max_{\\phi} \\phi^T \\pmb S \\phi - \\lambda (\\phi^T \\phi -1) \\text{  .}$$\n",
    "\n",
    "Solving this problem yields:\n",
    "\n",
    "$$ \\pmb S \\phi - \\lambda \\phi = \\pmb 0 \\iff  \\pmb S \\phi = \\lambda \\phi \\text{  .}$$\n",
    "\n",
    "Thus, $\\phi$ is an eigenvector of the covariance matrix $\\pmb S$ and the scalar $\\lambda$ denotes the corresponding eigenvalue. Since we are interested in the linear combination with the largest variance  $Z_1$, the respective loadings vector $\\phi_1$ (i.e. the eigenvector) is identified by the largest eigenvalue $\\lambda_1$ as \n",
    "\n",
    "$$ Var(\\tilde{\\pmb X} \\phi)  = \\phi^T \\pmb S \\phi = \\lambda \\phi^T \\phi = \\lambda \\, \\text{ ,}$$\n",
    "\n",
    "using the solution of the maximisation problem. Note that this solution of the maximisation problem is unaffected if the eigenvectors $\\phi$ is multiplied by (-1), so the actual signs of the loadings are meaningless - merely their respective magnitudes as well as their sign patterns matter. \n",
    "\n",
    "As $\\pmb S$ is a symmetric $P \\times P$ matrix, a total number of $P$ eigenvalues can be obtained. The corresponding eigenvectors $\\phi_j$ with $j=1, \\dots, P$ can be required to be orthogonal to each other, which is ensured when they satify $\\phi_{j}^T \\phi_{j\\,'} = 1$ for $j=j\\,'$ and zero otherwise. It can be shown that the remaining $2, \\dots, P$ linear combinations can be found as solutions to the problem of successively maximizing the variance subject to the constraint of uncorrelatedness with previously obtained linear combinations.\n",
    "\n",
    "In consequence, we are able to obtain $P$ linear combinations, i.e. principal components, of the $P$ centred explanatory variables - and all of these linear combinations are uncorrelated. Further, they are arranged in descending order according to their variance, which implies that the variablility of the original variables is more concentrated within the first couple of principal components. Dimensionality reduction now means, that not all of the principal components are used (otherwise no dimensionality reduction would have been achieved) but instead only the principal components up to $M < P$ are utilised. In a regression framework, this implies conducting Principal Component Regression (PCR): instead of regressing the outcome variable $Y$ on the original explanatory variables $\\pmb X$, we regress it on a number of principal components:\n",
    "\n",
    "$$ Y_i = \\gamma_0 + \\gamma_1 Z_{1i} + \\dots + \\gamma_M Z_{Mi} \\, \\text{ ,}$$\n",
    "\n",
    "The crucial assumption thereby is that the directions in which the (centred) original variables have the most variability are also the directions that are associated with the outcome variable $Y$. The word \"direction\" is used as the vectors of principal component loadings, $\\phi$, characterise a direction in a multi-dimensional space. Given that this assumption holds, using only $M$ linear combinations that contain most of the original statistical information improves upon the overfitting problem if $M << P$. Referring back to the variance-bias trade-off, we accept that a small part of the original statistical information is not used (i.e. introducting bias) while achieving a reduction of the variance of our predictions when being applied to a new random sample (test data set) which is likely to overcompensate for the introduced bias.  Further, interpretatibility of the model is increased as only a lower number of variables is included - which are also uncorrelated, so multicollinearity is no longer an issue.\n",
    "\n",
    "The questions remains, how many principal components should be included in the regression model. A helpful concept in this context is the *proportion of variance explained* (PVE). As already stated, we intend to reduce the dimensionality of the data while preserving as much as possible of the original variability. So the PVE states how much of the total variance is captured by an individual principal component. As the explanatory variables are standardised, the total variability is calculated as follows:\n",
    "\n",
    "$$ \\sum_{j=1}^{P} Var(\\tilde{X}_j) = \\sum_{j=1}^{P} \\frac{1}{N}\\sum_{i=1}^{N} \\tilde{x}_{ji}^2 \\text{ .}$$\n",
    "\n",
    "Further, the variance attributable to the $m$th principal component is \n",
    "\n",
    "$$ \\frac{1}{N} \\sum_{i=1}^{N} z_{mi}^2 = \\frac{1}{N} \\sum_{i=1}^{N} (\\sum_{j=1}^{P} \\phi_{mj} \\tilde{x}_{ji})^2 \\text{ .}$$\n",
    "\n",
    "Hence, the the proportion of total variance explained by the principal component $m$ is given by:\n",
    "\n",
    "$$ \\frac{\\sum_{i=1}^{N} (\\sum_{j=1}^{P} \\phi_{mj} \\tilde{x}_{ji})^2}{\\sum_{j=1}^{P} \\sum_{i=1}^{N} \\tilde{x}_{ji}^2} \\text{ .}$$\n",
    "\n",
    "Each PVE of the individual principal components is a positive number that lies between 0 and 1. In order to obtain the cumulative PVE up to prinipal component $M$, this would simply be the sum over the PVE of components $1, \\dots, M$. Both, the PVE and the cumulative are often visualized, where especially plotting the former against the number of principal components receives particular attention and is know as a \"Scree Plot\" (an example will be presented in a later section).\n",
    "\n",
    "To pin down the number of principal components to include in a PCR, a number of rather *ad-hoc* decision rules have been proposed. For instance, choosing only as many principal components as to explain a sizeable fraction of the total variance has been suggested. While it is of course context-dependent, which actual cut-off with respect to the cumulative PVE is appropriate, requiring the principal components to explain at least 70-90\\% of the total variance can be seen as common practice (see Jolliffe, 112-113). A more visual approach would be to analyse the Scree Plot with the PVE on the y-axis and the number of principal components on the x-axis. By eyeballing one would identify the last principal component after which the PVE of the following ones drops off, which is sometimes referred to as an *elbow* (Hastie/Tibshirani, 384). A last example of those rather ad-hoc decision rules would be to keep those principal components with a corresponding eigenvalue $\\lambda$ that is larger than the average eigenvalue $\\bar{\\lambda}$, or only larger than $0.7 \\bar{\\lambda}$ (Jolliffe, 115).\n",
    "\n",
    "A more systematic and, as some would argue, more objective way to identify the number of principal components to include is to choose them via Cross-Validation. As already stated, our aim is to identify an empirical model which is able to explain $Y$ well. Thus, a well fitted model should be able to give good predictions of the outcome when being applied to different data that is likewise randomly drawn from the population. Overfitting is a potential issue which could lead to a situation in which the model is able to predict $Y$ well on a data set the model was trained on, but does poorly in predicting the outcome when using a test data set. One way to achieve good predictive performance on test data is to use Cross-Validation (CV). Then, the available data is randomly split into $k$ folds, commonly $k = 10$. Initially, the first fold is kept as a validation set, and the model is fit on the rest of the data, i.e. on the other $(k-1)$ folds. The quality of the estimated model $\\hat{f}$ is evaluated on the held-out fold and the corresponding prediction error is calculated as the mean squared error (MSE). This procedure is repeated $k$ times, always keeping another fold as the validation set. The $k$-fold Cross-Validation estimate of the MSE is then simply the average of the obtained measures of the prediction errors: $MSE_1, \\dots, MSE_k$. Referring back to the problem of selecting the appropriate number of principal components, the number would be chosen in order to achieve the lowest CV MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1\n",
    "### Visualisation of the General Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach is best described when starting with low-dimensional data, such as $P = 2$, as it this can easily be visualised. For this purpose, data is drawn from a bivariate normal distribution in which the data generating process features both variables to have differing variances. Further, they have a positive covariance. The centred data as well as the two principal components are presented in the following graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'mvtnorm' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'gridExtra' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'cowplot' was built under R version 3.6.3\"\n",
      "********************************************************\n",
      "Note: As of version 1.0.0, cowplot does not change the\n",
      "  default ggplot2 theme anymore. To recover the previous\n",
      "  behavior, execute:\n",
      "  theme_set(theme_cowplot())\n",
      "********************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(mvtnorm)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(cowplot)\n",
    "\n",
    "#adjust size of graphs\n",
    "options(repr.plot.res = 80) \n",
    "# options(repr.plot.width=4, repr.plot.height=4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prco <- function(X) {\n",
    "    cov <- cov(X)\n",
    "    p <- dim(cov)[1]\n",
    "    \n",
    "    pca <- eigen(cov)\n",
    "    colnames(pca$vectors) <- paste(\"PC\", 1:p, sep=\"\")\n",
    "    rownames(pca$vectors) <- paste(\"X\", 1:p, sep=\"\")\n",
    "    \n",
    "    PVE <- pca$values / sum(apply(X, 2, var))\n",
    "    cPVE <- cumsum(PVE)\n",
    "    importance <- round(rbind(pca$values, PVE, cPVE), 4)\n",
    "    rownames(importance)[1] <- \"Var (Eigenval.)\"\n",
    "    colnames(importance) <- paste(\"PC\", 1:p, sep=\"\")\n",
    "    \n",
    "    vars <- cbind(paste(\"PC\", 1:p, sep=\"\"), pca$values)\n",
    "    colnames(vars) <- c(\"PC\", \"var\")\n",
    "    df_vars <- data.frame(vars)\n",
    "    df_vars$var <- as.numeric(as.character(df_vars$var))\n",
    "    \n",
    "    return(list(values=pca$values, vectors=pca$vectors, df_vars = df_vars, importance=importance))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAIwCAMAAACvL6FdAAAAGFBMVEUAAAAAAP8zMzNNTU3r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\nsQRgIJYADMQSgIFY+h/4WKgOqDFV4wAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bivariate Normal Distribution\n",
    "\n",
    "set.seed(1234)\n",
    "\n",
    "N <- 200\n",
    "sigma <- matrix(c(5, 6, 6, 12), byrow = T, nrow = 2)\n",
    "mu <- c(5, 15)\n",
    "\n",
    "X <- rmvnorm(N, mean = mu, sigma = sigma)\n",
    "colnames(X) <- c(\"X1\", \"X2\")\n",
    "X_demeaned <- scale(X, center=T, scale = F)\n",
    "\n",
    "df_demeaned <- as.data.frame(X_demeaned)\n",
    "\n",
    "pca <- prco(X_demeaned)\n",
    "\n",
    "p1 <-   ggplot(df_demeaned, aes(X1, X2)) + \n",
    "        xlim(-10, 10) +\n",
    "        geom_point() + \n",
    "        geom_abline(aes(slope = pca$vectors[2, 1] / pca$vectors[1, 1], intercept = 0, colour=\"PC1\")) + \n",
    "        geom_abline(aes(slope = pca$vectors[2, 2] / pca$vectors[1, 2], intercept = 0, colour=\"PC2\")) +\n",
    "        theme(plot.title = element_text(hjust = 0.5)) +\n",
    "        labs(color = \"\") + xlab(\"X1 (demeaned)\") + ylab(\"X2 (demeaned)\") +\n",
    "        scale_color_manual(values = c(\"PC1\" = \"red\", \"PC2\" = \"blue\")) +\n",
    "        ggtitle(\"Visualisation of Principal Components\") +\n",
    "        coord_fixed()\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive covariance of the data can be spotted as the point cloud has an upward slope. Since the first principal component is by definition the linear combination of the two variables which captures as much of their variability as possible, it also has a positive slope, thereby capturing the co-movement. The first principal component, depicted in the graph by the red line, has the property that it is the line which minimises the sum of the squared perpendicular distances between the data points and the line. The second principal component (blue line), by construction, must be orthogonal to the first one. Thus, in the two-dimension space, it is perpendicular to the first one. As can already been seen from the graph, the variability in the direction of the first principal component is in this case distinctly larger than the one in the direction of the second one. \n",
    "\n",
    "The numerical values of the principal component loadings are presented below. The first principal component assigns more weight to the second variable X2 than to the first variable X1 (0.86 vs. 0.5), as the former is also the one with a higher variance (see sample covariance matrix). And indeed, the first principal component captures far more of the total variability than the second one: about 92\\% of the total variability is explained by it and only around 8\\% by the second component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Sample Covariance Matrix:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>5.3894 </td><td> 6.6935</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>6.6935 </td><td>13.0390</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X1 & X2\\\\\n",
       "\\hline\n",
       "\tX1 & 5.3894  &  6.6935\\\\\n",
       "\tX2 & 6.6935  & 13.0390\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1 | X2 |\n",
       "|---|---|---|\n",
       "| X1 | 5.3894  |  6.6935 |\n",
       "| X2 | 6.6935  | 13.0390 |\n",
       "\n"
      ],
      "text/plain": [
       "   X1     X2     \n",
       "X1 5.3894  6.6935\n",
       "X2 6.6935 13.0390"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Principal Components:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>0.5019 </td><td>-0.8649</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>0.8649 </td><td> 0.5019</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tX1 & 0.5019  & -0.8649\\\\\n",
       "\tX2 & 0.8649  &  0.5019\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| X1 | 0.5019  | -0.8649 |\n",
       "| X2 | 0.8649  |  0.5019 |\n",
       "\n"
      ],
      "text/plain": [
       "   PC1    PC2    \n",
       "X1 0.5019 -0.8649\n",
       "X2 0.8649  0.5019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Importance of Components:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Var (Eigenval.)</th><td>16.9235</td><td>1.5050 </td></tr>\n",
       "\t<tr><th scope=row>PVE</th><td> 0.9183</td><td>0.0817 </td></tr>\n",
       "\t<tr><th scope=row>cPVE</th><td> 0.9183</td><td>1.0000 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tVar (Eigenval.) & 16.9235 & 1.5050 \\\\\n",
       "\tPVE &  0.9183 & 0.0817 \\\\\n",
       "\tcPVE &  0.9183 & 1.0000 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| Var (Eigenval.) | 16.9235 | 1.5050  |\n",
       "| PVE |  0.9183 | 0.0817  |\n",
       "| cPVE |  0.9183 | 1.0000  |\n",
       "\n"
      ],
      "text/plain": [
       "                PC1     PC2   \n",
       "Var (Eigenval.) 16.9235 1.5050\n",
       "PVE              0.9183 0.0817\n",
       "cPVE             0.9183 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAIwCAMAAACvL6FdAAAAElBMVEUAAAAA/wAzMzNNTU3r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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Sample Covariance Matrix:\")\n",
    "# Sample cov is invariant to demeaning\n",
    "round(cov(df_demeaned), 4)\n",
    "\n",
    "print(\"Principal Components:\")\n",
    "round(pca$vectors, 4)\n",
    "\n",
    "print(\"Importance of Components:\")\n",
    "pca$importance\n",
    "\n",
    "ggplot(data = pca$df_vars, aes(x=PC, y=var)) +\n",
    "    geom_bar(stat=\"identity\", fill=\"green\") + ylab(\"Variance (Eigenvalue)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, it shall be demonstrated that PCA with standardised data indeed leads to different principal components than when using data that is demeaned only. The following plot shows the demeaned data from before on the left side and the standardised version of the same underlying data on the right. As one can see, the slope of the principal components differ across panels. The first principal component with demeaned data assigned more weight to the second variable X2, which is captured by the corresponding red line having a slope larger than one, it seems that the first principal component in the case with standardised data has identical factor loadings for the two variables.\n",
    "\n",
    "This is because standardisation alters the covariance matrix while demeaning leaves is unaffected. As the principal components are obtained as the eigenvectors of the covariance matrix, they then also differ. The covariance matrix of the standardised data is in fact the correlation matrix of the underlying data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAGFBMVEUAAAAAAP8zMzNNTU3r\n6+vy8vL/AAD///8Gf59SAAAACXBIWXMAAAxNAAAMTQHSzq1OAAAgAElEQVR4nO2di5qrKhJG\nze6Mvv8bz4nxwq1+CiwUzb++mbO7hTbEYgneymEihIgMVzeAkJ6hIIQAKAghAApCCICCEAKg\nIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApC\nCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQA\nKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICC\nEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAgh\nAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKg\nIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApC\nCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQA\nKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICC\nEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAgh\nAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKgIIQAKAghAApCCICCEAKg\nIIQAKAghAApCCICC/A7Dl/2XdXmyMigTP0BYKK1kEH7uiV7bRexx+vxXjmH5XegEw5TuH2Kf\nQabliob04ss5pzEm+65DTb3Dzqo1riDOEgqCOEmQ/Z/6fVcLQbqKRWv2KAz+Ym9/tUkz78jm\nf7aa88/uf/fyb2Vv+bbQWe7+u3+u97fe+i/nfEGcJRTkVNKCBEEZ9v9u+zM/ZvtfDHv5Wjdc\nvi50/9Zb95T4W2f9l3O2IPX7rnD/tOyg9oWpndqtdlat2Se6giDfAG09OuyoXrC26v6/4XJ3\nXcGiSDzvb70PupArBSnbd8X7J2fnNUWFN9xZtWZI/LT86AZIFGTdist/h3BDTkO8PIzA6mha\nEKesl93WSYJY7LvS4ZB3XrfbWbUmLYh3/sTbRyX2HoO3LLWRg+XOVvbinwxK+Lc9BOUkQVKf\nV7Dv2geH1P5pi3CwU7vbzqo10bZ3Ou/6e0aQaHnuX2835P8Id3/R517FlYLo912JLR0vTO7U\nhBV1ubNqjbftlwWDX7brkonCsKzEWzbEy0sP0odg/ZdztiB1+y6VIMmdGFpRbzur1njfMZjy\nJgVZT24Mwd8EZ3CndaHz77D+wW6D6szJ+rle+ZWcL0jtvks4JEzvxG65syIdcrog1fsudz/k\nCRLulKTl4Yq621mRDmHHIARAQQgBUBBCABSEEAAFIQRAQQgBUJD78Z5Z/vmP19tleicRFj+o\n+rgttgQJ8vf5/9+f6eeR40S9wzOk4y7ctPq4L7YECPJRY3bEDUumyWeVW37AmCnPr980Inni\nFrykluW/0FOqO1E03diyIH+TI8i/f/9MP7YjxqsbUEyi1ziGdNuFm1Z393KmGzszxfqBESQ5\ngNxsBHEN6bULN63uBdF0Y/+8IGk/bifIbkinXbhpdT+Iphv71wUR/LifIJshfXbhptWDIJpu\nbP1ZLFWTzyqnIHELX7Dlt+rxZdXDGJpubP11EFWTzyq3+gDJjzsKshjSYxduWj2KoenGpiAG\n6zeNiDYSiRa+QMtv1OPLqschNN3Yvy2I6Mc9BZkN6a8LN62eCKHpxqYgBus3jYg2EskWvjrs\nwrXVh0FRPRVB043904LIftxVkP8M6bfHl1X/PDCdrZ6+C8KSXxJk3yWJo3Pd+k0joo2E0MJX\ntz2+sLpGEOEiryU/JIizxe8tCOZ1dQOsyGeaOeMuoR8WBPnRtSC4hdMrvbjsi96h+ihVt+SH\nBAmnWE8V5J005A49vqz6KFa35JcE8cuhH7cWJGnIZT3e3S1Zrn2Uq1vys4JgP+4tSMqQqwTx\nJraGax9BdUueIkgYhd8WJGHIwwQZUXVLHiJIFIbcCjJ+3F2Q2JBnTbFGWN0SClLXgN4FiQy5\nwVG3vvqIq1vyEEFKp1jSGUJ1A7oXJDSk7x5fVn0Ulq+LLXmKIIXlPyCILhtQHz2+rPooLN8W\nW/Kbgoin0OvWbxoRbSTynUyTDaiLHl9WfRSW74st+UlB5FPodes3jYg2EopOpsgG1EOPF5ev\n82ZvsZvfR1iLJb8lyLLFf0UQRTagngXZzry4i0ex+r7Ykp8SZNni4BpT3fpNI6KNhKqTZbMB\n3U2QUa6+L7aEghis3zQi2kjoOlkuG1DPgiSmWKN3tpKCWK9g8+M9JS9gVa7fNCLaSCg7WSYb\nUNeCRItH/3oXBWmxguUibPIWiMr1m0ZEGwltJ8PZgG4lyPimIMpyCqLvZDAb0J0E+USOUyxd\nef0KtrsUfmSK9cbZgHoVJCFCIr+PsBZLfkeQ/RSvRQPuJAjKBtSpIImpVCq/j7AWS35GEOcM\nlkUDbiUIyAZ0G0GS+X2EtVjyM69g+2zy6Y5vA1kJsyQX9UkxG9BlgiRnuGv14ABxKnvLkelm\nf+gIktj83gByvxEkyrNf1ielbEBXCZI+RyIKUvQSF9Pt/kxBEpt/XpS9z62u3DQigCPv+uos\nG9AyoMvF3q8XjvsUpK4BVwhyYIr1lrIB9TnFCih8R4XpRn+mIMIUK38jaF25aUREttcN4xaK\nfbIoX1ZPMzLx4R0KYryCWwuyv44bt1DukyX5smx6vHStqWjt8sM7FERbIQ5EagWKO6Xryk0j\nIvD3t53Gwi0EXbggX5aJIOLdCiVrB88mUBBlhUQgHidIFImaLqzPl9WNIOjWawqirKATRPMo\nQV25aUS0kajqwup8Wb1MsVD+KwqirqCZYo2Z8rIG3FQQdb6sTg7SYf4rCmK6Agoyo8yX1Ycg\n4Ma5zz6RghiuIHyXtuX6TSOijURtn9Tly+pCEHDj3DyrpiCGK0gLIt7z/lxBdPmyehAE3RdE\nQYxXEL1sfv6v/NTUgwVR5cvqQBB82wOnWKYriF82P//3NwXR5MtqJEgij49UXXFVty9BVHn6\nzioXKyRzjYmC/OQU663Jl9VGkHWHpKiuuWjVlyDvlyIN2VnlUoVkrrGCpzXryk0joogEbKGq\nC2fzZV0tiOqiVWeCvB1FKIj3y7ngFuq6cC6SF0+xdOfkuxNkV6RXQbLpXK0acHdBcvmyrj1I\nV56T71CQXBKZ3FeyKi9bAQVJLMa7uktHkPQpR31jLCk/i/VCOTJyX8mqvGgEKXjev67cNCLa\nSBzuwnBXd+UxSOKMSvpkSp+CzIr0Kkg23/G3EgX5gPJlXShI4oBROB3fqyBv74SWZsOYlx8R\nBF2EVTfgCYKgfFmmguy9WzHFSs2Hs4L4OR4sqb1QmDHkAkG+2yiREDyumBCk+DXS3i/ngltY\n0oXlfFmWgsTdG1RPz4czU6wgjZYl1VfS8SByviBxKqWFeIsnpljFb8l9hiByvqyrBCnJf9W3\nIHie1Y8gQkYMCvJFypfVaIqVq16U/6rrKdb8X9mQy6ZYYbk2Y8ydplimdJUvK8p/BZNnncKx\nmxXFQeSqg/SoPBBk84AH6evyomxAbWdkUX6fZWAvXbslR+/mFRTpRZDID+Fmxtr1L7+cC25h\ncRcuyQbUVJA4v88DBBEUoSANwS0s78IlAWwoSCq/j/rmX2+xJTlBwmxMqabpk8mYlStXkLxG\naNKAJwlSEsDiteev8DkvbjHSz5KsIEFYkk2LB5HTBUkeZA/SIToF8ZerswEVr30YUhf53Oru\ni1vuJ8g6gORyir8uPhkypLKFD+P1J0FaUNdrYHX1Hq5OEHSa91sB5vfp+F6s6K0UYtNepz5x\nGFaIwvApH1M7rzfvxUotV2YDqpli5dL6zX7AW1A6vxdLJ4jqSWez8uT5jqQgqTTvvBcrXq7L\nBqReu3vTVTat32eoh2vvVxD9CPL2BpETBVk3XXShcBSSMlCQ5HJVNiDt2hO37YIXnI+Zc4sd\nT7GiN+Phpr0yz6nl/l5dvldwHXA245R4rbZbjYJEy1Vz5GpBvH2VXz37WqOOD9KjsGSavCpy\niSBuDKTX2lk14IGCqObIJVMsb6CWBcnn93mSIJnX1iv+XlWe3PKeINgPCpJarkhbU7D2+cSi\n+2u6uiK/z7MEmQeRMw/SXUP2cgqi+0Le8vxBZL0gQnVNfp+HCfJR5BpBHDJ+UJD08mxeJ/3a\nYfbc7SdVfp/HCZJ/JtdYkLh8TJ/3MGvAQwXJZQNSrX3e9N/dFqzuv1YVrL0wP6zpxm6Wmxcb\nYiXIf5suPYAIZ86Xv6Eg4vJM4rPs2tcLg3lBtuvnubWLsby1IHgQMRJkSN/JAAWBkatroGlE\ntJFoJEgm8Vlu7XtA0vsh5+zvdv08t/aHCgIVsRHkGwt/6y13vk1gAKEgcDk8DakTJFjuXqna\n5XlHfvzUFGtGNMROkG3JmtXkM7OFV8o5xcosR5kBFVOsaHl0qer7X32+ywcepK/l0iBiNsXa\nFmz7pvnWN+n8olkDniwIyAZUtfboZofvXsxo7YnFljR/gU5aEcuzWF/WKMwDyDMF8W/6aSiI\nnA2obu3RFEnw4ycFSStiL4gThdH6ZsRcuWlERP7OE0TMBmTVhYXrVL8pSMkDa9pyVAG+e96q\nAecL8jepHl0zovEDcFF+n2455x2F0SBCQSo4cQQRc8tm1q5ML6bNVoYXP2cEeUeKmAiSPvsH\nH940a8DjBRHOQOK1R1cs0tWj/Fe6taurW3LeW25flk8cfiqkrx+NNh9AQYryZWUE8ZfKV6l+\nWhDV3dTa8pQgy9Wnd/AgQt0HUJCifFl4iuWcYXzLT3rm166tbsmp70k//hJQL69V6Mfnqvr4\nDu6zLttX1ZWbRkQbifaC4HxZ6VzhQZknyH59MHF/ELp8+zOCHH4J6L5dYwGG5Rqhv/kL91V1\n5aYR0UbiBEHQ6Ud/w/pricLk3uUwL47Corj5V992S04WZFXETBDPhTU9BgXRfyFcXc6XVSTI\ntnw7hRIPIBRk5chLQIOxO4zT+hQIp1jqL5SpLp+fL5hi7SRPoYCbf2EbnyqIxUtA4ynWfFtv\nmB9DfmqKgiir6/JlwbXvAoyp6vs+rvCu3ccKYvAS0KjCdgDilq/3jlZ8AAVZUV3ASizehoTl\nmQT3/qu0IKXPfTxYkEMvAQ0HY/cuXn8Fe2jKG1hSbhoRbSTOEiSTL8t5AipYvh5UDCvJ/FfO\nAz0UxC2vfgloeDi3n0N0H8DZdl0U5HB1dIV32bpIkHfkRxy99ZeiNj5bkOqXgKoF8S9PVTRQ\nXW4aEW0kzhMEXeEVBfkODc4IMnpFwd8XNCa/3HRjXygIUqRsivUdKDY/FoHEEVvfQGW5aUS0\nkThREJQvS5pivf1n2Hw/pFHj1oI0weSu6vUSofPr8sPk/fsU6nrNoeq5WyBygnj378p7Lh6D\nROWVLwH1KuzX0Ldf4xGmtoGactOIaCNxqiC5fFnpxcvlqOj5QdmPsqu6vyBI5UtAwxOF+ysm\n3tsUa/+Vghyvju8RQmsfgvxX8q2kvQsybJMTOSyZJleVJxQpE+Q/HEGio0aexbKoDvNlgbUH\nZ0/8W0ndxe/Op1jBxF0IS6bJleXFLwENK3h+eCMIbzWxqo5uopPXHvohCNL/vVhDvCgVlkyT\na8vDQST39/5w8c2kuC1xN3XpqF1VbhqRPLiFzQQBN9FFt+buq4nzJyanWP0LogxLpsn15UUv\nAY3uTsgJEllCQSqqS/mygr2Q8+sUpxFHU6zpnRzyuxBkuyEAhyXT5CPlL3A9ase/n335YfTu\neIunWPE4QkFqqod3maZvovIEibIkC4Ksn5oa8rsQZPn5ioP0DcVLQMMRYRlAImXCQFIQk+p+\nvixno3s7fmeKFb5p+FszPIJ0BpyOBRnCBamwZJp8sDz/EtD94pOzcNwXrNt3Wn7b/symgely\n04j0jXdld59wfOce8fRjXCYlbr1onf7Czi7r9iWI4iWgzt0L25xqXO/bDQSRL9neWhDcwpYj\nyH+d9+X/vv47eCFZi8bl2Vrt+N75QfpV10F8XqmTJYkDvXWzzqdJ9gVLTQqi/EIl1eftmbqu\nOyw7KPdi7Xz9fAqPAZO3brlTLIu2W9LNWSyH+GRJ1NNdQcbFC69SMMWybWBYbhoRbSQuEkQy\nZPvv+uN8/Xzyyk0bA6pb0qMg8UtAh8iQaT8y3E7xKm6cblNuGhFtJK6ZYn22cSYb0D6wK17i\neaAxl0yxrj2LtRNeOEwJso7jyYkUBVF+obLq3y2Nbg3aojHvtx4lyLD+D4Ul02S78sgQ/ydH\nkPb3WuXKTSOSB7fwjD65BSdxdL1MeMfwntFmjYkXW9KtINKN8N55kHkAadUAfblpRJyACCdN\ncAtP6ZNLbLbhIjothVMkUxCT8le0o3pHJwoH4YXo9xdEvnkUt7Bdn3SHhNeyxBHEO/Fue962\nsLppGLyQ9HCa1y1/BXH4/vKtMPhH6G0aoCs3jUgUmCgkuIXN+uTgHVSskdmqu9dBtmH9WYJg\nVE02L5/jEJ1gf3+zu0oHIGc2cP7lXHALTxJkjow7gjiCJPP72DYGVrekd0H+G0T8G31CQZo3\nQFFuGpGARIBwC9v0yfUarLP85Zx+96ZYyfw+lo3JVW+1/Qc4x1I1uUH58J8iTokrCHjU+TGC\niJE4VZDk0fjLu7Nn2/puULbq/kB/T0Eyg4mqyQ3K/wvB6zucr78vS4c9FvE97RRE+YXKBdm3\n8su5syc1fvg3a5k1Jlvdkv4FeX8v3np3Wm2XQJxfHibIsCJF4vwp1nf5tpndu072xYEfwU3W\nNo3JVTeNQ/LHFKomNyv/HIq4t1tN7gRrWfy0KZb0fA5uYetp/34viWPINrCEk15/3DdvTHqx\ndRDWH/s8Bll5vdwN/qmwByM+lfUAQcTHD3ALmx8Xe4NCkA0oPGlSeI6xb0EyqJrcpHzdyq+X\nd2P0GJZf1kDTiASh6UaQ6EZ1976sZXF00uRZZ7Ewqia3KHfmr94zuWNcfk0Df+JWk+AOhp09\nG9AQz6+Kj3BK2ni6IJ1OsVwBnGdyx/WYHQkiPhByB0FwJHoRZMsGtL0g0vkL3dqTJ5Hzbbzg\nIL2b293DKVakyDSM26VbWZCkO/YNNI1IHtzCllOsdI9fHnD7PnfgHpc/UZDQkL8///X1mSY3\nKvc6+veZ3MUPaQh5giD+iB5G4oKDdGlzvr4z3u3M9F77UVOslCB/y/+nkwUJAuFu9/e8yxqd\nZcueK/UBd55iZSLRUBDp6Dq1oWde03xEGJQvqyk4QuxbkMlJzxKG5d+/f6YfmyE6Flp2puvS\n12t02zqXdJYu5jhpQU6IhLwtnQ3tV/nEI1qYW9styLT9ohEkHsuD4XvMPZP7gLNYl40gYG60\n3Z4YbO9ReFfYhCbA6kb2MoIk6GSKtcTFESTxTG5Pgvwvpjw03jFID1Ms5/Zdv9vH4XDWnjDk\ndEHq45FJ2nCVIPFRnHcK8ZNwKXgmNwhDd4KMyoBInCmIvNg9BN8Wjt9wiKsZIkU6EEQbjzuc\nxfLuUvywJlxyFQmi0JsgY/kIEnD1Wax15IgKxqW6+KawITLkekHU8ej4mfT0aP7+XrFdTuM6\ninQtyFgxxRIv3OIWmgninpHa+nhcfdwWL09/xmvvTxB9PPoVZI1JtHX3jGSfEteQcxvo/JIL\nyFh1DCJduMUttBLEPSkyyIJ8kwJ8F7/EmW5nU6yCePSbtCESZJv9eoLE6YGiW98bNdD5JROQ\nTzwqBbnuZsVYkLW6d3g+OIJsj+3AtcuXR84SpCQeHd+s6PjhnVhcc/G+txtK46P11LlF8wbu\nv+CAzPG4nSD7FMvdnpM/SIxLDKZlUWhIYu3LSeRkhM4RpCgevd+sOPiCfH4a98XrRp4zOzh/\n0ZUg33isAVk38bqlwR4qceHWicRpB+neBnW69hA9svYZzyNBUmfsLxWkLB59PFGYOlG+H6EP\n3o5sdBZvG3nwDEmt8VgDcTkMyBKPLSDTNjDM/6+4yoxbqOxk2dufvAN0V5A9Mu4Ta2udl7fl\np4QIF0+xCuPRhSDxRnSf7PTu7lmDMoTmrPOsfSpg2MBMOQrIGo9EQCZ8+3TTB6bcOVOyuhMT\nd4+zV/deXzuts+Egb28siHAurKTt2eqW8ehekHdQc0/6ul+y+kr0clOZ9SLIFo9gSN/vIxM2\n9zZESpE4TxBhNW5W5GlZ6ee/bt7eaIr1Xe11ghTHo49jkGgnE89d1yP0dQXBQcf8w+vVmyB7\nPNw9FgpIOjSJSLSZYu2L05PUfbt7jw96cyYnX6w74OxLrxOkPB5djCA+y1FcvPDzb+o5W2fb\nb0+t9yGIE49EQIbMVr/gID03sDjnEsO1OBHZ8/ZO0d9dOsWqiMfNBAkz/URVXkGajRYNjMql\ngLjxCAKiOIt1xWlelSBeILa1uBFx8mWFKy5oTHV1y3gMws9SWDJNNikfUjeTzlvYyYqcmAG4\nV0V6EMSLx02ug2RObkk5kcPTUvHrvLPXD8FyI0Gq4uGOINcdg7gIY/AWl/wxxit+C6hlA+Py\ndED8eNxBEKcTy9W3U+3uX0TV8UB+iSB18ejuSrp4FPf1Yx3i8Qe8OhAkiEfVzYrp+OAW1ncy\ndxoEqq9+BBfYA+D77q8QpDIe9xHk7T2InvkA4f1tmgZUlKcCEsbj8O3uYSQuE+Q9brXRIQua\n6V4gSG08Mg9MRWHJNNmg3O//7tTVGUDyH5BRpLUgUTxKBRkSP3mROGuK5R/sTdvxuetHcu0v\nYbmyMQeqW8aj39vdZ7yTH6PiLIuzAmjIGQfpx0aQTt5RGJwunNInEpNrkQ8FrzlIf6AggxuE\n5Sas8ArUSrSng/Os0wV5KQPixkY4Z4JbaNonhyEUxDl/New3WafXsh4Kpm61q2iMurplPLoW\nxInOfuZkOde213XuagxXICtytiCvGxyDxIsjP4bw/O5SQ9hfvaa1jkFj9NUt49HvA1OBH6Ig\nS6Uglmu5pMjJgrwsD9JPI+oQn2fVElUSnWZZ+tp+vjY5Vn08ujuL5ZQ7Pd69dTTYZa210oJI\nhyLnCvKqOAbJReKEESQcP1Iv3RamWOvJxvU59dT4XtaYguqW8ehZEHfuOmzJ3KMV+Ldlx+XJ\nQeRUQT7xuKMg79AP5w7q3FrWk43uvb3HGqOvbhmP/S7GFRyWTJMNysPLHN8t6+SqLv+AhCJn\nCjLH4w6CZK6kf/P77E9+6NbuP6ejb8yB6pbxCO/FuvwYZB+yvZlTTpDMLi1S5ERBvvFIP+IJ\ndkhrPSkS9p0s2L7u+D2HYF6+7Ui1ax8SA/ilgpTFI7qb9+oRZBPEPfiO/AhXkI9YEKbzBFni\nkXzEE+yS1uc/LxPEOwIchEed82sfhoQhVwpSGI/uBNmmWN8w+H6Ej4A4f5TdpfmDyGmCrPFI\nP+Ipb/A1GpfdrBgIEvgRbX9p7UPKkAsFKY1Hl6d5v2dx91AM68uL9gq5CKU+wFXkLEG2eBQ/\ncrtWlyLR/CB93eLzv/v9V+/E5o/j4a0mOpN4nSDF8ejxLNa6uVdBFj8SNUo/4BU/yVPTwLBc\nDsgeD+ERz+wTt6c/D+LcyrON4dtzBoPzWKD/V6IgM8oJbnNByuORMOfSKzqTO5At15eiy1Nw\npMNU3PFRQxyP9COeVV+jYSfbZk++INvro5KrGdKvnfLW/kov1jXSTJCKeDheiDfHuWHJNNmi\nfO7+28/v+PKUW6H4A17wQQVVA6NyKSBuPJKPeOJJrQRuoaEg6xRrO/5IrgaFY1+uOgJsLEhN\nPHo8BtkvlC/DezCCHxJkfQnokQaG5UJAvHjU3u5+5RTru8C5v/2AIJ4h1whSFY8ej0H28rQg\njkF1H/AyfuIwHRA/HnW3u199N294E5w0xdKsXXGKpKkgdfHoW5D37ofyypSy/NVekCAeNY/c\nCtHBLbQWZL0J7jv7SiXL0K79lV6sb4y2umU8Ohfk7b5/2/IDcs/kHhYkjEfNrSZCcHALbQV5\n+35I56t0a8+eQ2woSG08ehFEHCCcXO5ureMNwIocFSSKR48jiPyo07Y4yt9+RJBENiDVagwE\nqY5HJ4LIA4R7hcqpZdEAZMhBQQzOJrc/BhEPKvbqwfODjh+6AT1cnknr10yQ+ni4p3mlm+Pc\nsGSaXFsuCrIfIb7NBUGDyEFBjnPCWay8IKnnB8OfyhqDT7I3E+R4HNDNcW5YMk2uLhemWKNU\ny6gBoiKXCwLALTScYoV+eM/eVN6/DvNe9i6IPO2dLjpIj0Nk3QBBkZ8QJFM9tfG36nVTrDfM\nBnQDQcCjw6omK8rDB6LQ34t+uCsoOmpMlCcNub8g+ic2hMXJjV8ygROqg8SwPQsCbo5zw5Jp\ncr5cvqUn/HvvGpX8AenzKiUNTA0iPQry50Ui02u058XFPhndIYqrh4A4y5dpuxYkh6rJ+XK1\nIMOwPYeeCJWlIClFOhTk71RBxvjaE7hkXiSIfJmWgrz1U6xhfh/6dkOp/AFHp1gzoSL9CfL3\nHUH+/funq38sz87oJPQZtul3yTpR3ZPuqbYgut29l4P0bQBJ3GfSpAG6G7KT5c2j9KVoBFEu\nFpavfnx+3iIA7kosbUxZtrI+RhD5wpQblkyT7crnAUS+dtugAS/N/abJctOIpPjOrs4TZLu9\n5/PLfqdPyRQr05iic4d9CALP8U7NBBGvzC6CmBxjKMtf+dvpkuWmEZFpIEh6SBjfriDvAb4I\npLIxJecOexEEH5Comlxc7o3jXvmWpczkGENZ/srdLZT8e9OIyNgI4m7O9M5nzn8VTGzXn80E\nSRrSuSBXjCDrfiolyHt/fqf+A0rLa14CahqRPMLnDT8AAA9NSURBVLiFmU7mbeikIKO/FtXJ\nsJrG6E+u9yHIRccg+xDil+9pZg5+QGl5xUtATSOSB38DhSCuIVH10V+L7p6Sqsao82V1Ichl\nZ7GCp2mXcuc1X0c/oLi8+CWgphHJg79BfooFe/zoLw4jYClIbEjPgijDkmnywfL9WNB9Dtrw\nA5TlhY/kmkZEG4nqg/TknGldOAbVldcbKxujzJdFQVa2cMg3YbVtwErZS0BNI6KNRLUgqeXr\nhh+j6rpbumobo7s8S0E2wh2Z+Qeoy0teAmoaEW0kWggyKqsXrh0sVl2efbgg6ssYe8Uxt/72\ngmRGkY4EMdnHB36cJogqX9azBVHfFOpU7EIQ+ExuP4IcPkrYt7qqeuHas4sV9y9QkKDimF3/\nOYJoXwJqGhFtJKwESR/3nSeIIl/WswUpn2KN+fWfJIjyJaCmEdFGwmqKFR2f4+qFa9cszt7g\n83BBist7EkRSpCNBdN8sN8Uaw8VGa9cszuXL6lmQv7/1OR1Vky3KR8XfnyiI4q4h04jkwd+g\nrpOFpw1PFSSXL6trQYKwZJpsUd6dINmXgJpGJA/+BlWdLDqtfq4gmTvgOhZkHUDUz7EdZzzr\ng0p49fQMnE2fdJfHl51OFgTny+pZkGkbRFRNPl4+ZsqbN0Aof4FLWqYRyYO/QUUnS1yWPVsQ\nmC+rV0GWow8K8kW+pGUakTz4G5R3Mpj/6vDatYtBvqxeBZlOHEGCe0x6FER+CahpRPLgb1Dc\nyUryXzUUBOTL6liQ6S9INpNpcm15eLWqS0HEl4CaRiQP/gYFnWxOzJO+7e0CQeR7qHsWJAxL\npsm15dvVqvDR52tud5fLX6nzLaYR0UbieJ+cN7pwW+gVgoj5sijIJsL6vs5pW3zBA1O4PHG+\nxTQi5/HZuOP+i1jrpOZMXebLulSQqPOP6wNsHQuSeAlow/jIkTCZYm13vYlPDrZ8ojBeXpZJ\n/OkjSNT7x3coSHdTrJnwJaCmEdFGorhPpvMzuII4NS4SpCyT+K8Kosjde60g4WzZNCLaSJT2\nmsRg7N6zEBiyr6XlI7eJ5QXZgB4vSLhP215H2L8gQRrGc8Et1Avi5/eRBNGt3a56Qar9xwsS\nsGYyUfz95YJ4LwE1jYg2EoenWPEl2dQUS7t2u+rqbEC/JojmgbamDSgt39MwngtuobZP4itO\nFwqizgb0Y4KoHmhr2YCK8vWiyLngFir7ZJjfR/n9T6muzAZEQWw/oEX5kobxXHALdX0yzu+j\n+/7nVFe+rOWnBNn9uMVB+lY+p2E8F9xCVZ/M5ve5VhDly1p+U5D5XIrw9/mzLBeUn/6sCG6h\npk9K+X16OIv1RZMN6KcEcQcQURDnPGRHgtxvBJFOhyi277TXVXzoAZ80bzP6TUG2KVZ85ZeC\nOJGo75Pi0V6BILoL7EcGnHw2oF8SJJEzIHUbVpdTrLsJAvL76KdY7QXJZwP6IUGC8UMUpFkD\njpWbRkQbido+qcvvM+Xe7NV6ivXOZwP6SUGcmxWRHxSktk+C/D7uBp/K3g3Z5pg+9z68xwki\njeDeAHKXW03cX84FtxD3SZDfJ7ibtwNBcu/De5og4jHg6FXSrJ+C1PVJlN8nvN295OWpjQTB\n2YB+RpCCrDPHGtCq3DQi2khILURPmOEt7U+xFN/7hOrwhZFPE0SYYpVknTnYgEblphHRRkJo\noXRu41O9YEv3IgjKBvQ8QdLlFKQQ2EIgSEl+n24EAdmAfkSQorRMLRpwvNw0IhJhAqbiKVZR\nfp9+BAFvVKUgJh/Qvtw0IgJRCr/SXlOW36cjQcRsQL8hSFnesgYNMCg3jQjgI0htGvEuc4Ir\nuTIbEAUxKG8YHxfdFCu9XHzr4w1GECkb0E+MIIWJ/ewbYFFuGpEUf84Eq0oQ+a2PtxBEeJ1q\nX4I04c4D/8mErzIq6WTgpXb3EKQoX5bpZr94BBEGEI4gEX/hy/AKOhl6Z9dNBCnJl2W63a8V\nRPKDguQjoe9kY3oxXE1/1fX5skw3NgUxKDeNiDYS6k6GX0l0G0H0+bJMN/algoh+UJB8JLSd\nLPPGlfsIos6XZbqxKYhBuWlEtJFQdrJM/qs7CaLNl2W6sa8URPaDguQjoetkufxXtxJEmS/L\ndGNfKAjwg4LkI6HqZNn8V/cSRJcvy3RjUxCDctOIaCOh6WRS/qv8F+21uiZflunGvk4Q5AcF\nyUdC0ck06cCv7vGl1RX5skw3NgUxKDeNiDYS+U6mynZ8eY8vrZ7Pl2W6sS8TZI2e/PzbwQ84\nsdw0ItpIZDsZyH+VX95x9Wy+LNONfZUgux8luQEMG2BZbhoRbSRynUyX/6qHHl9aPZcvy3Rj\nUxCDctOIaCOR6WRS/qtwc/fQ40urZ/JlmW7siwTxX3ZQ/vcUJNPJpPxX0Q6pix5fWh3nyzLd\n2JcLUvf3FAR3MjH/1TMEwfmyTDf2NYKIz7dp109BYCcD+a+eMMV643xZphubghiUm0ZEGwnQ\nyUoSKfXS40urg3xZphv7EkHkB0C166cgoJMV5YnppseXVpfzZZlu7CsEAQ+AatdPQeROVpYG\no58eX1pdzJdlurFvJIjqjS2XlJtGRBsJqYWFaTA66vGl1aV8WaYb+wJB0BPS4O/98y+/LAjk\nl9JgnJEvi4IYlDeMjxyJdAvFwzvDfXzi0tVFA46QDciS8wWBKQTQ33OK5UUi2UJ59mrXhVM3\nP1w1I0tnA7LkdEFwCgHt+ilIsoVgcH6mIOlsQJZQEINy04hoI5FoIRqcHznFeqezAVlytiCZ\nHBva9VOQRAvhvucOp6WqqieyAVlCQQzKTSOijUTUQrxp79PjS6vH2YAsOVmQXBIa7fopSNTC\nzKa9UY8vrR5lA7LkYkGkd6FTkGwkghbm8vvcqceXVg+zAVmCBJlz7gcv/so0OVMeRlF6oR4F\nQaRamM3vc6seX1o9yAZkCRDko0b04q9Mk3F5FEUKUkOiBfn8PvNi/fmnewkSZAOyRBbkb3IE\nqX3xl098G8Rw8ftJbkncaxT5fT6LC65gXN3jS6t72YAsyUyxTEcQRRozbTlHELcFmvw+DxfE\nywZkSVqQ74EHBVGWm0YkT9gCVX6fZ0+x3l42IEtOHEE0ef605RRkb4Euv08HXbhx9T0bkCUn\nnsWiIEb4LZDy+yi/0IOqb9mALDnvOogqEaa2nIKsLZDy+2i/0JOqr9mALDlNEF0iTG05BVla\nIOb30X6hR1VfsgFZQkEMyk0joo3E/A/I76P8Qs+q/s0GZEl7Qb5nTpSZYrXlFGRuQUF+n166\ncOPqczYgS5oLspx7pyB2bC0oye/TTRduXP11T0GUp1rU5T8oSHg+8Zv/6rlX/mqrv24myDvl\nBwUpJroiNX39iAzpvws3rm6b6+Scg3QKYoJ3V9x8Y9tHkLM+/TaYGnKKIOpzkeryXxTEn2It\n6Y05xUostoSCGJSbRiTF4oYzxZLTG9+iC7etbskZguhP1qvLf0yQKTwGAdlbb9GF21a3hIIY\nlJtGRMI9i4WSU96iC7etbskJgpS8rUJb/oOCOJGAySlv0YXbVrekvSBFb6vQlv+0IDj33i26\ncNvqllAQg3LTiGgj0Vef7Kq6Jc0FKXudi7acgnTWJ7uqbgkFMSg3jYg2En31ya6qW9JakML3\nHWnLKUhnfbKr6pZU3qigTQIkve/ocBKhoyu4+u+P0GOf7Kq6JY0FafX3lzeAgnRc3RIKcs3f\nH6ewBaxeB+8FvStddbI7V8dQkLvSVSe7c3UMBSEEQEEIAVQJ4qdcrFnBob8++OHHP//w9ye3\noUaQ4MUhNWuo/svtz48JduzjD39/chsqBPmbjnaQwwPIkQ8//PnHv78FhV+h+BuXVG+68ubf\nFFM7xTomyJE/7uPvrxak8POLm1vSy5quvPk3zVAoSOrFIYX4z1fXreLg3xt8/tWCTOWfX9Yr\nG/bJopVvn9CuOoQjSN3fXy9I02lN451206b3McU6fBar/o+v//urz2IVDsJ/ZT14myWo/6Cs\neuPaNX8A4HWQe1I+ran4gEZtKT1iKV41BSGFI9hf8Ym7bs5ilTa9h7NYhPwKFIQQAAUhBEBB\nCAFQEEIAXQkybP8dEssqVtfVtyN3pK8utLqxvxVm8P4pXltfX4/cj7560CKIs+tfdPn+8y3+\n/PM1aHD/GcKFFMQOeWyfhI08iCX3orOvMARTqtWYwf9x2IaHIayzLxz8eJIjiGP7hAR5wObv\n7RsEU6phijyIlkypcqcaMUAa2wf3P84oTkHaIIwg3wFhGIbUkskriKsRC1BkEqP4OtLcfvv3\n9QW2Li2MIGiJWI2YIIzt3j/Jof3e9PUFcoIkp1j7notTrHakRpBtYhWO4nu922//rr7AHoJw\nprtNeL2eH52w8hauESMGJHddy7/i3ml6QAB6/wIH29f717sNmbE9OYq71W9L91/gUAO7/3Z3\nQR7bp2k98+6P4vvIcm/u/w3INah6zv271/2/AbkIRdd5QO96wFcgpB0UhBAABSEEQEEIAVAQ\nQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEE\nQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAU\nhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEI\nAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAA\nBSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQ\nQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEE\nQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAU\nhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEI\nAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAA\nBSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQQgAUhBAABSEEQEEIAVAQ\nQgAUhBAABSEE8H9inUxNV4iaBQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_std <- scale(X, center = T, scale = T)\n",
    "df_std <- as.data.frame(X_std)\n",
    "pca_std <- prco(X_std)\n",
    "\n",
    "p2 <-   ggplot(df_std, aes(X1, X2)) + \n",
    "        geom_point() + \n",
    "        geom_abline(aes(slope = pca_std$vectors[2, 1] / pca_std$vectors[1, 1], intercept = 0, colour=\"PC1\")) + \n",
    "        geom_abline(aes(slope = pca_std$vectors[2, 2] / pca_std$vectors[1, 2], intercept = 0, colour=\"PC2\")) +\n",
    "        theme(plot.title = element_text(hjust = 0.5)) +\n",
    "        labs(color = \"\") + xlab(\"X1 (stand.)\") + ylab(\"X2 (stand.)\") +\n",
    "        scale_color_manual(values = c(\"PC1\" = \"red\", \"PC2\" = \"blue\")) +\n",
    "        ggtitle(\"PCA with standardised data\") +\n",
    "        coord_fixed()\n",
    "\n",
    "p1 <- p1 + ggtitle(\"PCA with demeaned data\")\n",
    "\n",
    "options(repr.plot.width=10, repr.plot.height=8)\n",
    "grid.arrange(p1, p2, nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the correlation matrix is displayed. Both variables have a high correlation of about .8 and this leads to factor loadings of the first principal component that equally weight both of the standardised variables. Still, the first component explains about 90\\% of the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Sample Covariance Matrix:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>5.389441 </td><td> 6.693512</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>6.693512 </td><td>13.039028</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X1 & X2\\\\\n",
       "\\hline\n",
       "\tX1 & 5.389441  &  6.693512\\\\\n",
       "\tX2 & 6.693512  & 13.039028\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1 | X2 |\n",
       "|---|---|---|\n",
       "| X1 | 5.389441  |  6.693512 |\n",
       "| X2 | 6.693512  | 13.039028 |\n",
       "\n"
      ],
      "text/plain": [
       "   X1       X2       \n",
       "X1 5.389441  6.693512\n",
       "X2 6.693512 13.039028"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Covariance Matrix of Standardised Data (Correlation Matrix)\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>1.0000000</td><td>0.7984717</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>0.7984717</td><td>1.0000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X1 & X2\\\\\n",
       "\\hline\n",
       "\tX1 & 1.0000000 & 0.7984717\\\\\n",
       "\tX2 & 0.7984717 & 1.0000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1 | X2 |\n",
       "|---|---|---|\n",
       "| X1 | 1.0000000 | 0.7984717 |\n",
       "| X2 | 0.7984717 | 1.0000000 |\n",
       "\n"
      ],
      "text/plain": [
       "   X1        X2       \n",
       "X1 1.0000000 0.7984717\n",
       "X2 0.7984717 1.0000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Principal Component Loadings:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>0.7071068 </td><td>-0.7071068</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>0.7071068 </td><td> 0.7071068</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tX1 & 0.7071068  & -0.7071068\\\\\n",
       "\tX2 & 0.7071068  &  0.7071068\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| X1 | 0.7071068  | -0.7071068 |\n",
       "| X2 | 0.7071068  |  0.7071068 |\n",
       "\n"
      ],
      "text/plain": [
       "   PC1       PC2       \n",
       "X1 0.7071068 -0.7071068\n",
       "X2 0.7071068  0.7071068"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Importance of Principal Components:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Var (Eigenval.)</th><td>1.7985</td><td>0.2015</td></tr>\n",
       "\t<tr><th scope=row>PVE</th><td>0.8992</td><td>0.1008</td></tr>\n",
       "\t<tr><th scope=row>cPVE</th><td>0.8992</td><td>1.0000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tVar (Eigenval.) & 1.7985 & 0.2015\\\\\n",
       "\tPVE & 0.8992 & 0.1008\\\\\n",
       "\tcPVE & 0.8992 & 1.0000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| Var (Eigenval.) | 1.7985 | 0.2015 |\n",
       "| PVE | 0.8992 | 0.1008 |\n",
       "| cPVE | 0.8992 | 1.0000 |\n",
       "\n"
      ],
      "text/plain": [
       "                PC1    PC2   \n",
       "Var (Eigenval.) 1.7985 0.2015\n",
       "PVE             0.8992 0.1008\n",
       "cPVE            0.8992 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAElBMVEUAAAAA/wAzMzNNTU3r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\nB/P3zS0f2W+vAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Sample Covariance Matrix:\")\n",
    "cov(df_demeaned)\n",
    "\n",
    "print(\"Covariance Matrix of Standardised Data (Correlation Matrix)\")\n",
    "# Standardisation alters original Cov Matrix\n",
    "cov(df_std)\n",
    "\n",
    "print(\"Principal Component Loadings:\")\n",
    "pca_std$vectors\n",
    "\n",
    "print(\"Importance of Principal Components:\")\n",
    "pca_std$importance\n",
    "\n",
    "ggplot(data = pca_std$df_var, aes(x=PC, y=var)) +\n",
    "    geom_bar(stat=\"identity\", fill=\"green\") + ylab(\"Variance (Eigenvalue)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this point more clearly, consider the bivariate case with uncorrelated variables and drastically differing variances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"****************************\"\n",
      "[1] \"*** Demeaned Data **********\"\n",
      "[1] \"****************************\"\n",
      "[1] \"Sample Covariance Matrix:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>11.43334373</td><td>-0.07325336</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>-0.07325336</td><td> 1.86232795</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X1 & X2\\\\\n",
       "\\hline\n",
       "\tX1 & 11.43334373 & -0.07325336\\\\\n",
       "\tX2 & -0.07325336 &  1.86232795\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1 | X2 |\n",
       "|---|---|---|\n",
       "| X1 | 11.43334373 | -0.07325336 |\n",
       "| X2 | -0.07325336 |  1.86232795 |\n",
       "\n"
      ],
      "text/plain": [
       "   X1          X2         \n",
       "X1 11.43334373 -0.07325336\n",
       "X2 -0.07325336  1.86232795"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Principal Components:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>-0.999970715</td><td>-0.007652994</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td> 0.007652994</td><td>-0.999970715</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tX1 & -0.999970715 & -0.007652994\\\\\n",
       "\tX2 &  0.007652994 & -0.999970715\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| X1 | -0.999970715 | -0.007652994 |\n",
       "| X2 |  0.007652994 | -0.999970715 |\n",
       "\n"
      ],
      "text/plain": [
       "   PC1          PC2         \n",
       "X1 -0.999970715 -0.007652994\n",
       "X2  0.007652994 -0.999970715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Var (Eigenval.)</th><td>11.4339</td><td>1.8618 </td></tr>\n",
       "\t<tr><th scope=row>PVE</th><td> 0.8600</td><td>0.1400 </td></tr>\n",
       "\t<tr><th scope=row>cPVE</th><td> 0.8600</td><td>1.0000 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tVar (Eigenval.) & 11.4339 & 1.8618 \\\\\n",
       "\tPVE &  0.8600 & 0.1400 \\\\\n",
       "\tcPVE &  0.8600 & 1.0000 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| Var (Eigenval.) | 11.4339 | 1.8618  |\n",
       "| PVE |  0.8600 | 0.1400  |\n",
       "| cPVE |  0.8600 | 1.0000  |\n",
       "\n"
      ],
      "text/plain": [
       "                PC1     PC2   \n",
       "Var (Eigenval.) 11.4339 1.8618\n",
       "PVE              0.8600 0.1400\n",
       "cPVE             0.8600 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"****************************\"\n",
      "[1] \"*** Standardised Data ******\"\n",
      "[1] \"****************************\"\n",
      "[1] \"Covariance Matrix:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X1</th><th scope=col>X2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td> 1.00000000</td><td>-0.01587497</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td>-0.01587497</td><td> 1.00000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & X1 & X2\\\\\n",
       "\\hline\n",
       "\tX1 &  1.00000000 & -0.01587497\\\\\n",
       "\tX2 & -0.01587497 &  1.00000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X1 | X2 |\n",
       "|---|---|---|\n",
       "| X1 |  1.00000000 | -0.01587497 |\n",
       "| X2 | -0.01587497 |  1.00000000 |\n",
       "\n"
      ],
      "text/plain": [
       "   X1          X2         \n",
       "X1  1.00000000 -0.01587497\n",
       "X2 -0.01587497  1.00000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Principal Components:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>X1</th><td>-0.7071068</td><td>-0.7071068</td></tr>\n",
       "\t<tr><th scope=row>X2</th><td> 0.7071068</td><td>-0.7071068</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tX1 & -0.7071068 & -0.7071068\\\\\n",
       "\tX2 &  0.7071068 & -0.7071068\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| X1 | -0.7071068 | -0.7071068 |\n",
       "| X2 |  0.7071068 | -0.7071068 |\n",
       "\n"
      ],
      "text/plain": [
       "   PC1        PC2       \n",
       "X1 -0.7071068 -0.7071068\n",
       "X2  0.7071068 -0.7071068"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Var (Eigenval.)</th><td>1.0159</td><td>0.9841</td></tr>\n",
       "\t<tr><th scope=row>PVE</th><td>0.5079</td><td>0.4921</td></tr>\n",
       "\t<tr><th scope=row>cPVE</th><td>0.5079</td><td>1.0000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & PC1 & PC2\\\\\n",
       "\\hline\n",
       "\tVar (Eigenval.) & 1.0159 & 0.9841\\\\\n",
       "\tPVE & 0.5079 & 0.4921\\\\\n",
       "\tcPVE & 0.5079 & 1.0000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 |\n",
       "|---|---|---|\n",
       "| Var (Eigenval.) | 1.0159 | 0.9841 |\n",
       "| PVE | 0.5079 | 0.4921 |\n",
       "| cPVE | 0.5079 | 1.0000 |\n",
       "\n"
      ],
      "text/plain": [
       "                PC1    PC2   \n",
       "Var (Eigenval.) 1.0159 0.9841\n",
       "PVE             0.5079 0.4921\n",
       "cPVE            0.5079 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAGFBMVEUAAAAAAP8zMzNNTU3r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\n3G3S5j0h9wAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "N <- 200\n",
    "sigma <- matrix(c(12, 0, 0, 2), byrow = T, nrow = 2)\n",
    "mu <- c(5, 15)\n",
    "\n",
    "X <- rmvnorm(N, mean = mu, sigma = sigma)\n",
    "colnames(X) <- c(\"X1\", \"X2\")\n",
    "X_demeaned <- scale(X, center=T, scale = F)\n",
    "\n",
    "df_demeaned <- as.data.frame(X_demeaned)\n",
    "\n",
    "print(\"****************************\")\n",
    "print(\"*** Demeaned Data **********\")\n",
    "print(\"****************************\")\n",
    "print(\"Sample Covariance Matrix:\")\n",
    "cov(df_demeaned)\n",
    "print(\"Principal Components:\")\n",
    "pca <- prco(X_demeaned)\n",
    "pca$vectors\n",
    "pca$importance\n",
    "\n",
    "p1 <-   ggplot(df_demeaned, aes(X1, X2)) + \n",
    "        xlim(-10, 10) + ylim(-5, 5) +\n",
    "        geom_point() + \n",
    "        geom_abline(aes(slope = pca$vectors[2, 1] / pca$vectors[1, 1], intercept = 0, colour=\"PC1\")) + \n",
    "        geom_abline(aes(slope = pca$vectors[2, 2] / pca$vectors[1, 2], intercept = 0, colour=\"PC2\")) +\n",
    "        theme(plot.title = element_text(hjust = 0.5)) +\n",
    "        labs(color = \"\") + xlab(\"X1 (demeaned)\") + ylab(\"X2 (demeaned)\") +\n",
    "        scale_color_manual(values = c(\"PC1\" = \"red\", \"PC2\" = \"blue\")) +\n",
    "        ggtitle(\"PCA with demeaned data\") +\n",
    "        coord_fixed()\n",
    "\n",
    "print(\"****************************\")\n",
    "print(\"*** Standardised Data ******\")\n",
    "print(\"****************************\")\n",
    "X_std <- scale(X, center = T, scale = T)\n",
    "df_std <- as.data.frame(X_std)\n",
    "pca_std <- prco(X_std)\n",
    "\n",
    "print(\"Covariance Matrix:\")\n",
    "cov(df_std)\n",
    "print(\"Principal Components:\")\n",
    "pca_std$vectors\n",
    "pca_std$importance\n",
    "\n",
    "p2 <-   ggplot(df_std, aes(X1, X2)) + \n",
    "        geom_point() + \n",
    "        geom_abline(aes(slope = pca_std$vectors[2, 1] / pca_std$vectors[1, 1], intercept = 0, colour=\"PC1\")) + \n",
    "        geom_abline(aes(slope = pca_std$vectors[2, 2] / pca_std$vectors[1, 2], intercept = 0, colour=\"PC2\")) +\n",
    "        theme(plot.title = element_text(hjust = 0.5)) +\n",
    "        labs(color = \"\") + xlab(\"X1 (stand.)\") + ylab(\"X2 (stand.)\") +\n",
    "        scale_color_manual(values = c(\"PC1\" = \"red\", \"PC2\" = \"blue\")) +\n",
    "        ggtitle(\"PCA with standardised data\") +\n",
    "        coord_fixed()\n",
    "\n",
    "plot_grid(p1, p2, align = \"h\", nrow = 1, rel_heights = c(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As X1 is the variable that has a much larger variance than X2 and the sample covariance between them is almost zero, the first principal component computed based on the demeaned data loads almost exclusively on X1. In other word, the first principal component is in the direction of X1, hence the red line in the left panel is essentially a flat horizontal line. About 86\\% of the total variance is captured by it. In turn, the second principal component must be orthogonal which yields a vertical line. \n",
    "\n",
    "While the point cloud of the standardised data looks the same, the standardisation procedure takes out the difference in variance: the newly generated variables have both a variance of 1. Since the correlation between the variables is almost zero, it follows that the factor loadings of the first principal component obtained with standardised data assign almost equal weight to both of the standardised variables.\n",
    "\n",
    "Thus, in the case of the covariance matrix approach, variables with a high variance will dominate the first principal component and one should be aware of this issue. For instance, it would actually matter whether length is measured in miles or in kilometres as this has implications for the variance of the measurements. With explanatory variables that are measured on different scales, it is therefore crucial to standardise in order to achieve scale invariance. However, one might be inclined to always standardise the variables, even if they are measured on the same scale, just to be precautious. However, the differences in variance may in fact be relevant and those high-variance variables might be more important when predicting the outcome. Hence, it is necessary to study the role of PCA in a regression context rather than as a tool for exploratory data analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Regression (PCR)\n",
    "\n",
    "To demonstrate the general procedure, consider the following set-up. We always draw a test and a training data set of equal size. The explanatory variables are obtained *Explain DGP*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_x <- function(N, mu) {\n",
    "  \n",
    "  p <- length(mu) # number of predictors\n",
    "  \n",
    "  # Q <- qr.Q(qr(matrix(rnorm(p^2), p))) # create an orthogonal matrix from a matrix of iid draws from N(0,1)\n",
    "  # sigma <- t(Q) %*% diag(sample(1:10, p, replace = T)) %*% Q # create Cov matrix\n",
    "  \n",
    "  W = replicate(p, rnorm(p))\n",
    "  S = W%*%t(W) + diag(runif(p),nrow=p)\n",
    "  S = diag(1/sqrt(diag(S)))%*%S%*%diag(1/sqrt(diag(S))) # correlation matrix\n",
    "  \n",
    "  sdevs <- sample(1:2, p, replace=T)\n",
    "  sigma <- diag(sdevs) %*% S %*% diag(sdevs)\n",
    "  \n",
    "  X_train <- rmvnorm(n=N, mean=mu, sigma=sigma)  # randomly draw training data set\n",
    "  X_test <- rmvnorm(n=N, mean=mu, sigma=sigma)  # randomly draw test data set\n",
    "  \n",
    "  return(list(X_train=X_train, X_test = X_test, sigma = sigma))   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_y <- function(intercept, effects, X_train, X_test, sigma_x, PC, random, eps_sd) {\n",
    "  p <- dim(sigma_x)[1]\n",
    "  N_train <- dim(X_train)[1]\n",
    "  N_test <- dim(X_test)[1]\n",
    "  \n",
    "  if (random == 1) {\n",
    "\n",
    "    beta_vec <- sample(effects, size= p, replace=T)\n",
    "    y_train <- rep(intercept, N_train) + X_train %*% beta_vec + rnorm(n=N_train, mean=0, sd=eps_sd)\n",
    "    y_test <- rep(intercept, N_test) + X_test %*% beta_vec + rnorm(n=N_test, mean=0, sd=eps_sd)\n",
    "  \n",
    "  } else {\n",
    "    stopifnot(length(effects) == length(PC))\n",
    "    V <- eigen(sigma_x)$vectors[,PC]\n",
    "    y_train <- rep(intercept, N_train) + X_train %*% V %*% effects + rnorm(n=N_train, mean=0, sd=eps_sd)\n",
    "    y_test <- rep(intercept, N_test) + X_test %*% V %*% effects + rnorm(n=N_test, mean=0, sd=eps_sd)\n",
    "  }\n",
    "  return(list(y_train = y_train, y_test = y_test)) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_cv <- function(X, Y, k) {\n",
    "  \n",
    "  stopifnot(nrow(Y) == nrow(X))\n",
    "  \n",
    "  data <- cbind(Y, X)\n",
    "  \n",
    "  fold_i <- sample(rep(1:k, length.out = nrow(X)))\n",
    "  \n",
    "  mse <- matrix(NA, nrow=ncol(X), ncol=k)\n",
    "  \n",
    "  for (x in 1:k) {\n",
    "    val_i <- which(fold_i == x)\n",
    "    train <- data[-val_i,]\n",
    "    val <- data[val_i,]\n",
    "    \n",
    "    pca <- prco(train[,-1])\n",
    "    \n",
    "    for (j in 1:ncol(X)) {\n",
    "      Z_train <- cbind(rep(1, nrow(train)),train[,-1] %*% pca$vectors[,1:j])\n",
    "      Z_val <- cbind(rep(1, nrow(val)),val[,-1] %*% pca$vectors[,1:j])\n",
    "      # Z_train <- train[,-1] %*% pca$vectors[,1:j]\n",
    "      # Z_val <- val[,-1] %*% pca$vectors[,1:j]\n",
    "      \n",
    "      beta_hat <- solve(t(Z_train)%*%Z_train) %*% t(Z_train) %*% train[,1]\n",
    "      mse[j, x] <- mean((Z_val %*% beta_hat - val[,1]) ** 2)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  mse_cv <- cbind(1:ncol(X), rowMeans(mse))\n",
    "  colnames(mse_cv) <- c(\"M\", \"CV-MSE\")\n",
    "  \n",
    "  M <- match(min(mse_cv[,\"CV-MSE\"]), mse_cv[,\"CV-MSE\"])\n",
    "  pca_full_train <- prco(X)\n",
    "  cv_vectors <- pca_full_train$vectors[,1:M]\n",
    "  \n",
    "  Z <- cbind(rep(1, nrow(X)),X %*% cv_vectors)\n",
    "  beta_hat <- solve(t(Z)%*%Z) %*% t(Z) %*% Y\n",
    "  mse_train <- mean((Z %*% beta_hat - Y) ** 2)\n",
    "  \n",
    "  importance <- pca_full_train$importance[,1:M]\n",
    "  \n",
    "  return(list(mse_cv = mse_cv, mse_train = mse_train, beta_hat = beta_hat, M=M, cv_vectors=cv_vectors, importance=importance))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N <- 500\n",
    "p <- 20\n",
    "set.seed(3)\n",
    "X <- gen_x(N = N, sample(10:20, p, replace = T))\n",
    "X_train_d <- scale(X$X_train, center = T, scale = F)\n",
    "X_test_d <- scale(X$X_test, center = T, scale = F)\n",
    "\n",
    "y  <- gen_y(intercept=2, effects=c(5, 3), X_train = X$X_train, X_test = X$X_test, sigma_x=X$sigma, PC = c(1, 2),random = 0, eps_sd=4) \n",
    "\n",
    "y_train <- y$y_train\n",
    "y_test <- y$y_test\n",
    "\n",
    "pca <- prco(X_train_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>PC1</th><th scope=col>PC2</th><th scope=col>PC3</th><th scope=col>PC4</th><th scope=col>PC5</th><th scope=col>PC6</th><th scope=col>PC7</th><th scope=col>PC8</th><th scope=col>PC9</th><th scope=col>PC10</th><th scope=col>PC11</th><th scope=col>PC12</th><th scope=col>PC13</th><th scope=col>PC14</th><th scope=col>PC15</th><th scope=col>PC16</th><th scope=col>PC17</th><th scope=col>PC18</th><th scope=col>PC19</th><th scope=col>PC20</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Var (Eigenval.)</th><td>10.2389</td><td>7.8445 </td><td>7.1248 </td><td>5.7580 </td><td>5.2181 </td><td>4.0953 </td><td>3.7574 </td><td>2.0135 </td><td>1.8863 </td><td>1.4748 </td><td>1.3106 </td><td>1.0661 </td><td>0.6174 </td><td>0.5423 </td><td>0.3658 </td><td>0.2337 </td><td>0.2003 </td><td>0.0669 </td><td>0.0379 </td><td>0.0257 </td></tr>\n",
       "\t<tr><th scope=row>PVE</th><td> 0.1900</td><td>0.1456 </td><td>0.1322 </td><td>0.1069 </td><td>0.0968 </td><td>0.0760 </td><td>0.0697 </td><td>0.0374 </td><td>0.0350 </td><td>0.0274 </td><td>0.0243 </td><td>0.0198 </td><td>0.0115 </td><td>0.0101 </td><td>0.0068 </td><td>0.0043 </td><td>0.0037 </td><td>0.0012 </td><td>0.0007 </td><td>0.0005 </td></tr>\n",
       "\t<tr><th scope=row>cPVE</th><td> 0.1900</td><td>0.3356 </td><td>0.4679 </td><td>0.5747 </td><td>0.6716 </td><td>0.7476 </td><td>0.8173 </td><td>0.8547 </td><td>0.8897 </td><td>0.9171 </td><td>0.9414 </td><td>0.9612 </td><td>0.9727 </td><td>0.9827 </td><td>0.9895 </td><td>0.9939 </td><td>0.9976 </td><td>0.9988 </td><td>0.9995 </td><td>1.0000 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllll}\n",
       "  & PC1 & PC2 & PC3 & PC4 & PC5 & PC6 & PC7 & PC8 & PC9 & PC10 & PC11 & PC12 & PC13 & PC14 & PC15 & PC16 & PC17 & PC18 & PC19 & PC20\\\\\n",
       "\\hline\n",
       "\tVar (Eigenval.) & 10.2389 & 7.8445  & 7.1248  & 5.7580  & 5.2181  & 4.0953  & 3.7574  & 2.0135  & 1.8863  & 1.4748  & 1.3106  & 1.0661  & 0.6174  & 0.5423  & 0.3658  & 0.2337  & 0.2003  & 0.0669  & 0.0379  & 0.0257 \\\\\n",
       "\tPVE &  0.1900 & 0.1456  & 0.1322  & 0.1069  & 0.0968  & 0.0760  & 0.0697  & 0.0374  & 0.0350  & 0.0274  & 0.0243  & 0.0198  & 0.0115  & 0.0101  & 0.0068  & 0.0043  & 0.0037  & 0.0012  & 0.0007  & 0.0005 \\\\\n",
       "\tcPVE &  0.1900 & 0.3356  & 0.4679  & 0.5747  & 0.6716  & 0.7476  & 0.8173  & 0.8547  & 0.8897  & 0.9171  & 0.9414  & 0.9612  & 0.9727  & 0.9827  & 0.9895  & 0.9939  & 0.9976  & 0.9988  & 0.9995  & 1.0000 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | PC1 | PC2 | PC3 | PC4 | PC5 | PC6 | PC7 | PC8 | PC9 | PC10 | PC11 | PC12 | PC13 | PC14 | PC15 | PC16 | PC17 | PC18 | PC19 | PC20 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| Var (Eigenval.) | 10.2389 | 7.8445  | 7.1248  | 5.7580  | 5.2181  | 4.0953  | 3.7574  | 2.0135  | 1.8863  | 1.4748  | 1.3106  | 1.0661  | 0.6174  | 0.5423  | 0.3658  | 0.2337  | 0.2003  | 0.0669  | 0.0379  | 0.0257  |\n",
       "| PVE |  0.1900 | 0.1456  | 0.1322  | 0.1069  | 0.0968  | 0.0760  | 0.0697  | 0.0374  | 0.0350  | 0.0274  | 0.0243  | 0.0198  | 0.0115  | 0.0101  | 0.0068  | 0.0043  | 0.0037  | 0.0012  | 0.0007  | 0.0005  |\n",
       "| cPVE |  0.1900 | 0.3356  | 0.4679  | 0.5747  | 0.6716  | 0.7476  | 0.8173  | 0.8547  | 0.8897  | 0.9171  | 0.9414  | 0.9612  | 0.9727  | 0.9827  | 0.9895  | 0.9939  | 0.9976  | 0.9988  | 0.9995  | 1.0000  |\n",
       "\n"
      ],
      "text/plain": [
       "                PC1     PC2    PC3    PC4    PC5    PC6    PC7    PC8    PC9   \n",
       "Var (Eigenval.) 10.2389 7.8445 7.1248 5.7580 5.2181 4.0953 3.7574 2.0135 1.8863\n",
       "PVE              0.1900 0.1456 0.1322 0.1069 0.0968 0.0760 0.0697 0.0374 0.0350\n",
       "cPVE             0.1900 0.3356 0.4679 0.5747 0.6716 0.7476 0.8173 0.8547 0.8897\n",
       "                PC10   PC11   PC12   PC13   PC14   PC15   PC16   PC17   PC18  \n",
       "Var (Eigenval.) 1.4748 1.3106 1.0661 0.6174 0.5423 0.3658 0.2337 0.2003 0.0669\n",
       "PVE             0.0274 0.0243 0.0198 0.0115 0.0101 0.0068 0.0043 0.0037 0.0012\n",
       "cPVE            0.9171 0.9414 0.9612 0.9727 0.9827 0.9895 0.9939 0.9976 0.9988\n",
       "                PC19   PC20  \n",
       "Var (Eigenval.) 0.0379 0.0257\n",
       "PVE             0.0007 0.0005\n",
       "cPVE            0.9995 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Mean Eigenvalue (Variance)\"\n",
      "[1] 2.693905\n",
      "[1] \"0.7 * (Mean Eigenvalue)\"\n",
      "[1] 1.885733\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAD1BMVEUAAAAzMzNNTU3r6+v/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\nRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PVE <- pca$importance[\"PVE\",]\n",
    "cPVE <- pca$importance[\"cPVE\",]\n",
    "PC <- 1:p\n",
    "\n",
    "p1 <- ggplot(data.frame(PC, PVE), aes(x = PC, y = PVE)) + \n",
    "      theme(plot.title = element_text(hjust = 0.5)) +\n",
    "      geom_point() + geom_line() + ylab(\"Proportion of Variance Explained (PVE)\") +\n",
    "      ggtitle(\"Scree Plot\") + scale_x_continuous(breaks = 1:p)\n",
    "\n",
    "p2 <- ggplot(data.frame(PC, cPVE), aes(x = PC, y = cPVE)) + \n",
    "      theme(plot.title = element_text(hjust = 0.5)) +\n",
    "      geom_point() + geom_line() + ylab(\"cum. Prop. of Variance Explained\") +\n",
    "      ggtitle(\"Cumulative Prop. of Variance Explained\") + scale_x_continuous(breaks = 1:p)\n",
    "\n",
    "plot_grid(p1, p2, align = \"h\", nrow = 1, rel_heights = c(1, 1))\n",
    "\n",
    "pca$importance\n",
    "print(\"Mean Eigenvalue (Variance)\")\n",
    "print(mean(pca$values))\n",
    "print(\"0.7 * (Mean Eigenvalue)\")\n",
    "print(0.7 * mean(pca$values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the first XX PCs, yields about XX\\% of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those ad-hoc rules, the decision would be done without any consideration of the outcome variable (\"unsupervised learning\"). As we are interested in predicting the outcome, a supervised learning technique would be, as described above, to choose the number of principal components to include based on Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of Principal Components chosen by CV:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "7"
      ],
      "text/latex": [
       "7"
      ],
      "text/markdown": [
       "7"
      ],
      "text/plain": [
       "[1] 7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>M</th><th scope=col>CV-MSE</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1      </td><td>76.87639</td></tr>\n",
       "\t<tr><td> 2      </td><td>26.67639</td></tr>\n",
       "\t<tr><td> 3      </td><td>20.86771</td></tr>\n",
       "\t<tr><td> 4      </td><td>18.26644</td></tr>\n",
       "\t<tr><td> 5      </td><td>18.05803</td></tr>\n",
       "\t<tr><td> 6      </td><td>17.74511</td></tr>\n",
       "\t<tr><td> 7      </td><td>17.43739</td></tr>\n",
       "\t<tr><td> 8      </td><td>17.58211</td></tr>\n",
       "\t<tr><td> 9      </td><td>17.67557</td></tr>\n",
       "\t<tr><td>10      </td><td>17.82422</td></tr>\n",
       "\t<tr><td>11      </td><td>17.94314</td></tr>\n",
       "\t<tr><td>12      </td><td>17.94498</td></tr>\n",
       "\t<tr><td>13      </td><td>18.06848</td></tr>\n",
       "\t<tr><td>14      </td><td>18.28188</td></tr>\n",
       "\t<tr><td>15      </td><td>18.32267</td></tr>\n",
       "\t<tr><td>16      </td><td>18.39550</td></tr>\n",
       "\t<tr><td>17      </td><td>18.48188</td></tr>\n",
       "\t<tr><td>18      </td><td>18.57943</td></tr>\n",
       "\t<tr><td>19      </td><td>18.62130</td></tr>\n",
       "\t<tr><td>20      </td><td>18.62953</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{ll}\n",
       " M & CV-MSE\\\\\n",
       "\\hline\n",
       "\t  1       & 76.87639\\\\\n",
       "\t  2       & 26.67639\\\\\n",
       "\t  3       & 20.86771\\\\\n",
       "\t  4       & 18.26644\\\\\n",
       "\t  5       & 18.05803\\\\\n",
       "\t  6       & 17.74511\\\\\n",
       "\t  7       & 17.43739\\\\\n",
       "\t  8       & 17.58211\\\\\n",
       "\t  9       & 17.67557\\\\\n",
       "\t 10       & 17.82422\\\\\n",
       "\t 11       & 17.94314\\\\\n",
       "\t 12       & 17.94498\\\\\n",
       "\t 13       & 18.06848\\\\\n",
       "\t 14       & 18.28188\\\\\n",
       "\t 15       & 18.32267\\\\\n",
       "\t 16       & 18.39550\\\\\n",
       "\t 17       & 18.48188\\\\\n",
       "\t 18       & 18.57943\\\\\n",
       "\t 19       & 18.62130\\\\\n",
       "\t 20       & 18.62953\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| M | CV-MSE |\n",
       "|---|---|\n",
       "|  1       | 76.87639 |\n",
       "|  2       | 26.67639 |\n",
       "|  3       | 20.86771 |\n",
       "|  4       | 18.26644 |\n",
       "|  5       | 18.05803 |\n",
       "|  6       | 17.74511 |\n",
       "|  7       | 17.43739 |\n",
       "|  8       | 17.58211 |\n",
       "|  9       | 17.67557 |\n",
       "| 10       | 17.82422 |\n",
       "| 11       | 17.94314 |\n",
       "| 12       | 17.94498 |\n",
       "| 13       | 18.06848 |\n",
       "| 14       | 18.28188 |\n",
       "| 15       | 18.32267 |\n",
       "| 16       | 18.39550 |\n",
       "| 17       | 18.48188 |\n",
       "| 18       | 18.57943 |\n",
       "| 19       | 18.62130 |\n",
       "| 20       | 18.62953 |\n",
       "\n"
      ],
      "text/plain": [
       "      M  CV-MSE  \n",
       " [1,]  1 76.87639\n",
       " [2,]  2 26.67639\n",
       " [3,]  3 20.86771\n",
       " [4,]  4 18.26644\n",
       " [5,]  5 18.05803\n",
       " [6,]  6 17.74511\n",
       " [7,]  7 17.43739\n",
       " [8,]  8 17.58211\n",
       " [9,]  9 17.67557\n",
       "[10,] 10 17.82422\n",
       "[11,] 11 17.94314\n",
       "[12,] 12 17.94498\n",
       "[13,] 13 18.06848\n",
       "[14,] 14 18.28188\n",
       "[15,] 15 18.32267\n",
       "[16,] 16 18.39550\n",
       "[17,] 17 18.48188\n",
       "[18,] 18 18.57943\n",
       "[19,] 19 18.62130\n",
       "[20,] 20 18.62953"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_pcr <- pcr_cv(X=X_train_d, Y=y_train, k=10)\n",
    "\n",
    "print(\"Number of Principal Components chosen by CV:\")\n",
    "my_pcr$M\n",
    "\n",
    "my_pcr$mse_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'pls' was built under R version 3.6.3\"\n",
      "Attaching package: 'pls'\n",
      "\n",
      "The following object is masked from 'package:stats':\n",
      "\n",
      "    loadings\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>344.572380008311</dd>\n",
       "\t<dt>1 comps</dt>\n",
       "\t\t<dd>75.5433799181424</dd>\n",
       "\t<dt>2 comps</dt>\n",
       "\t\t<dd>26.7871169874408</dd>\n",
       "\t<dt>3 comps</dt>\n",
       "\t\t<dd>20.9902400087495</dd>\n",
       "\t<dt>4 comps</dt>\n",
       "\t\t<dd>18.0553525673167</dd>\n",
       "\t<dt>5 comps</dt>\n",
       "\t\t<dd>17.9166435720242</dd>\n",
       "\t<dt>6 comps</dt>\n",
       "\t\t<dd>17.4006234543775</dd>\n",
       "\t<dt>7 comps</dt>\n",
       "\t\t<dd>17.2325268746784</dd>\n",
       "\t<dt>8 comps</dt>\n",
       "\t\t<dd>17.3812977157894</dd>\n",
       "\t<dt>9 comps</dt>\n",
       "\t\t<dd>17.3957043547066</dd>\n",
       "\t<dt>10 comps</dt>\n",
       "\t\t<dd>17.4558005414742</dd>\n",
       "\t<dt>11 comps</dt>\n",
       "\t\t<dd>17.5238856471538</dd>\n",
       "\t<dt>12 comps</dt>\n",
       "\t\t<dd>17.5813164372651</dd>\n",
       "\t<dt>13 comps</dt>\n",
       "\t\t<dd>17.6111920659452</dd>\n",
       "\t<dt>14 comps</dt>\n",
       "\t\t<dd>17.7556459008436</dd>\n",
       "\t<dt>15 comps</dt>\n",
       "\t\t<dd>17.782317181319</dd>\n",
       "\t<dt>16 comps</dt>\n",
       "\t\t<dd>17.7928088920526</dd>\n",
       "\t<dt>17 comps</dt>\n",
       "\t\t<dd>17.8799193126638</dd>\n",
       "\t<dt>18 comps</dt>\n",
       "\t\t<dd>17.9328259656994</dd>\n",
       "\t<dt>19 comps</dt>\n",
       "\t\t<dd>17.9126857568891</dd>\n",
       "\t<dt>20 comps</dt>\n",
       "\t\t<dd>17.9016753775622</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] 344.572380008311\n",
       "\\item[1 comps] 75.5433799181424\n",
       "\\item[2 comps] 26.7871169874408\n",
       "\\item[3 comps] 20.9902400087495\n",
       "\\item[4 comps] 18.0553525673167\n",
       "\\item[5 comps] 17.9166435720242\n",
       "\\item[6 comps] 17.4006234543775\n",
       "\\item[7 comps] 17.2325268746784\n",
       "\\item[8 comps] 17.3812977157894\n",
       "\\item[9 comps] 17.3957043547066\n",
       "\\item[10 comps] 17.4558005414742\n",
       "\\item[11 comps] 17.5238856471538\n",
       "\\item[12 comps] 17.5813164372651\n",
       "\\item[13 comps] 17.6111920659452\n",
       "\\item[14 comps] 17.7556459008436\n",
       "\\item[15 comps] 17.782317181319\n",
       "\\item[16 comps] 17.7928088920526\n",
       "\\item[17 comps] 17.8799193126638\n",
       "\\item[18 comps] 17.9328259656994\n",
       "\\item[19 comps] 17.9126857568891\n",
       "\\item[20 comps] 17.9016753775622\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   344.5723800083111 comps\n",
       ":   75.54337991814242 comps\n",
       ":   26.78711698744083 comps\n",
       ":   20.99024000874954 comps\n",
       ":   18.05535256731675 comps\n",
       ":   17.91664357202426 comps\n",
       ":   17.40062345437757 comps\n",
       ":   17.23252687467848 comps\n",
       ":   17.38129771578949 comps\n",
       ":   17.395704354706610 comps\n",
       ":   17.455800541474211 comps\n",
       ":   17.523885647153812 comps\n",
       ":   17.581316437265113 comps\n",
       ":   17.611192065945214 comps\n",
       ":   17.755645900843615 comps\n",
       ":   17.78231718131916 comps\n",
       ":   17.792808892052617 comps\n",
       ":   17.879919312663818 comps\n",
       ":   17.932825965699419 comps\n",
       ":   17.912685756889120 comps\n",
       ":   17.9016753775622\n",
       "\n"
      ],
      "text/plain": [
       "(Intercept)     1 comps     2 comps     3 comps     4 comps     5 comps \n",
       "  344.57238    75.54338    26.78712    20.99024    18.05535    17.91664 \n",
       "    6 comps     7 comps     8 comps     9 comps    10 comps    11 comps \n",
       "   17.40062    17.23253    17.38130    17.39570    17.45580    17.52389 \n",
       "   12 comps    13 comps    14 comps    15 comps    16 comps    17 comps \n",
       "   17.58132    17.61119    17.75565    17.78232    17.79281    17.87992 \n",
       "   18 comps    19 comps    20 comps \n",
       "   17.93283    17.91269    17.90168 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Principal Components chosen by CV in pls package:\"\n",
      "7 comps \n",
      "      7 \n"
     ]
    }
   ],
   "source": [
    "library(pls)\n",
    "\n",
    "data <- cbind(y_train, X$X_train)\n",
    "colnames(data) <- c(\"y_train\", paste(\"X\", 1:p, sep=\"\"))\n",
    "df_train_pcr <- data.frame(data)\n",
    "pcr.fit <- pcr(y_train~., data=df_train_pcr, scale=F, center=T, validation=\"CV\")\n",
    "MSEP(pcr.fit)$val[1,,]\n",
    "M <- which.min(MSEP(pcr.fit)$val[1,,]) - 1\n",
    "\n",
    "print(\"Principal Components chosen by CV in pls package:\")\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My CV function seems to work quite ok, differences can be due to random effects in how CV is conducted\n",
    "\n",
    "Evaluate Performance on test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Test-MSE of my PCR function\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "18.5864689619207"
      ],
      "text/latex": [
       "18.5864689619207"
      ],
      "text/markdown": [
       "18.5864689619207"
      ],
      "text/plain": [
       "[1] 18.58647"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Test-MSE of the PCR function of the pls package:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "410.683938207108"
      ],
      "text/latex": [
       "410.683938207108"
      ],
      "text/markdown": [
       "410.683938207108"
      ],
      "text/plain": [
       "[1] 410.6839"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Test-MSE of OLS\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "16.9268345609888"
      ],
      "text/latex": [
       "16.9268345609888"
      ],
      "text/markdown": [
       "16.9268345609888"
      ],
      "text/plain": [
       "[1] 16.92683"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse_test_pcr <- mean((cbind(rep(1, nrow(X_test_d)), X_test_d %*% my_pcr$cv_vectors) %*% my_pcr$beta_hat -y_test) ** 2)\n",
    "\n",
    "pcr.pred = predict(pcr.fit, X$X_test, ncomp = M)[1]\n",
    "mse_pcr_c <- mean((pcr.pred - y_test) ** 2)\n",
    "\n",
    "X_t <- cbind(rep(1, nrow(X$X_train)), X$X_train)\n",
    "beta_ols <- solve(t(X_t) %*% X_t) %*% t(X_t) %*% y_train\n",
    "mse_test_ols <- mean((cbind(rep(1, nrow(X$X_test)), X$X_test) %*% beta_ols - y_test)**2)\n",
    "\n",
    "print(\"Test-MSE of my PCR function\")\n",
    "mse_test_pcr\n",
    "print(\"Test-MSE of the PCR function of the pls package:\")\n",
    "mse_pcr_c\n",
    "print(\"Test-MSE of OLS\")\n",
    "mse_test_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MSE of the package is implausibly high. Test MSE of my PCR is higher than OLS - not good. Why? Although DGP is based on only the first two principal components, actually the first 7 are selected using CV... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_func <- function(N, P, intercept, effects, PC, random_effects, eps_sd, k) {\n",
    "  X <- gen_x(N = N, sample(10:20, size=P, replace = T))\n",
    "  X_train_d <- scale(X$X_train, center = T, scale = F)\n",
    "  X_test_d <- scale(X$X_test, center = T, scale = F)\n",
    "  \n",
    "  y  <- gen_y(intercept=intercept, effects=effects, PC = PC,random = random_effects, eps_sd=eps_sd, X_train = X$X_train, X_test = X$X_test, sigma_x=X$sigma) \n",
    "  y_train <- y$y_train\n",
    "  y_test <- y$y_test\n",
    "  \n",
    "  pcr <- pcr_cv(X=X_train_d, Y=y_train, k=k)\n",
    "  Z <- cbind(rep(1, nrow(X$X_test)),X_test_d  %*% pcr$cv_vectors)\n",
    "  mse_test_pcr <- mean((Z %*% pcr$beta_hat - y_test) ** 2)\n",
    "  \n",
    "  X_t <- cbind(rep(1, nrow(X$X_test)), X$X_train)\n",
    "  beta_ols <- solve(t(X_t) %*% X_t) %*% t(X_t) %*% y_train\n",
    "  mse_test_ols <- mean((cbind(rep(1, nrow(X$X_test)), X$X_test) %*% beta_ols - y_test) ** 2)\n",
    "  \n",
    "  return(list(mse_test_ols = mse_test_ols, mse_test_pcr = mse_test_pcr,  M = pcr$M))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps <- 50\n",
    "res <- matrix(NA, nrow=15, ncol=4)\n",
    "colnames(res) <- c(\"true M\", \"MSE_OLS\", \"MSE_PCR\", \"selected M\")\n",
    "reps <- 50\n",
    "\n",
    "for (m in 1:15) {\n",
    "  simulation <-\n",
    "    replicate(reps, sim_func(\n",
    "      N = 50, \n",
    "      P = 20, \n",
    "      intercept= 0, \n",
    "      effects = sample(5:10, size=m, replace=T), \n",
    "      PC = 1:m, \n",
    "      random_effects = 0, \n",
    "      eps_sd = 2, \n",
    "      k = 5))\n",
    "  \n",
    "  sim <-  mapply(simulation, FUN = as.numeric)\n",
    "  sim <- matrix(data = sim, ncol = reps)\n",
    "  mean_values <- rowMeans(sim)\n",
    "  \n",
    "  res[m, 1] <- m\n",
    "  res[m, 2] <- mean_values[1]\n",
    "  res[m, 3] <- mean_values[2]\n",
    "  res[m, 4] <- mean_values[3]\n",
    "  \n",
    "}\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asset Index as Empirical Application\n",
    "\n",
    "Motivation:\n",
    "\n",
    "- survey data collected in developing countries often does not contain information on hh's income or consumption expenditures\n",
    "    - see Demographic and Health Survey (DHS): conducted in broad range of developing countries, \"nationally-representative household surveys that provide data for a wide range of monitoring and impact evaluation indicators in the areas of population, health, and nutrition\", frequently used in development studies\n",
    "    - generally no information on income or expenditures, therefore a wealth index is constructed as \"a composite measure of a household's cumulative living standard\", \"using easy-to-collect data on a household's ownership of selected assets, such as televisions and bicycles; materials used for housing construction; and types of water access and sanitation facilities. \n",
    "    \n",
    "Aim:\n",
    "\n",
    "- Construct \"asset index\" to proxy for \"a household's long-run economic status\" (Filmer/Pritchett 2001)\n",
    "    - constructed usually from data on asset ownership or housing characteristics (e.g. Filmer/Pritchett 2001)\n",
    "    \n",
    "Choosing appropriate weights?\n",
    "\n",
    "- equal weights (simply sum over the variables in the case of Dummy variables) has the \"appeal of simplicity\" but this \"masks the fact that the imposition of numeric equality is completely arbitrary\" (Filmer/Pritchett 2001)\n",
    "- \"simply adding all asset variables separately in a linear regression would implicitly create weights\", but most assets may exert both an indirect (as a proxy of wealth) but also a direct effect, thus it is not possible to isolate the effect of an increase in wealth (Filmer/Pritchett 2001)\n",
    "\n",
    "Standardization of variables. Based on the index, build percentiles: bottom 40% poor, upper 20% rich\n",
    "\n",
    "Crucial assumption:\n",
    "\n",
    "- hh's \"long-run wealth explains the maximum variance (and covariance) in the asset variables\", \"no way to test this assumption directly\" (Filmer/Pritchett 2001)\n",
    "- \"internal/external coherence\" (Filmer/Pritchett 2001):\n",
    "    - internal coherence: \"poor\" should have lower mean of assets associated with high wealth and high means for assets associated with low wealth\n",
    "    - external coherence: comparison across Indian states using state-level poverty rates\n",
    "- compare with consumption expenditures, when both available in the survey (Filmer/Pritchett 2001)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- in case of dummy variables: change from 0 to 1\n",
    "\n",
    "Factor Analysis:\n",
    "\n",
    "- Filmer/Pritchett (2001): list FA as alternative approach and state that the ranks derived under both approaches has 0.988 Spearman rank correlation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>wealth1</th><th scope=col>taste1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>0</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>0</td></tr>\n",
       "\t<tr><td>1</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " wealth1 & taste1\\\\\n",
       "\\hline\n",
       "\t 0 & 1\\\\\n",
       "\t 0 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 0 & 0\\\\\n",
       "\t 0 & 0\\\\\n",
       "\t 1 & 0\\\\\n",
       "\t 1 & 0\\\\\n",
       "\t 1 & 0\\\\\n",
       "\t 1 & 0\\\\\n",
       "\t 1 & 0\\\\\n",
       "\t 1 & 0\\\\\n",
       "\t 1 & 0\\\\\n",
       "\t 1 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| wealth1 | taste1 |\n",
       "|---|---|\n",
       "| 0 | 1 |\n",
       "| 0 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 0 | 0 |\n",
       "| 0 | 0 |\n",
       "| 1 | 0 |\n",
       "| 1 | 0 |\n",
       "| 1 | 0 |\n",
       "| 1 | 0 |\n",
       "| 1 | 0 |\n",
       "| 1 | 0 |\n",
       "| 1 | 0 |\n",
       "| 1 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "   wealth1 taste1\n",
       "1  0       1     \n",
       "2  0       1     \n",
       "3  1       1     \n",
       "4  1       1     \n",
       "5  1       1     \n",
       "6  1       1     \n",
       "7  1       1     \n",
       "8  1       1     \n",
       "9  1       1     \n",
       "10 1       1     \n",
       "11 0       0     \n",
       "12 0       0     \n",
       "13 1       0     \n",
       "14 1       0     \n",
       "15 1       0     \n",
       "16 1       0     \n",
       "17 1       0     \n",
       "18 1       0     \n",
       "19 1       0     \n",
       "20 1       0     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "taste1 <- c(rep(1, 10), rep(0, 10))\n",
    "wealth1 <- c(0, 0, rep(1, 8), 0, 0, rep(1, 8))\n",
    "sit1 <- cbind(wealth1, taste1)\n",
    "df1 <- as.data.frame(sit1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAKACAMAAABTxewAAAAAD1BMVEUAAAAzMzNNTU3r6+v/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\n586cQJw7c/8HBMaeBsd4wKgAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(ggplot2)\n",
    "ggplot(df1, aes(wealth1, taste1)) +\n",
    "  geom_point(position = position_jitter(w = 0.01, h = 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>wealth1</th><th scope=col>taste1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>wealth1</th><td>0.1684211</td><td>0.0000000</td></tr>\n",
       "\t<tr><th scope=row>taste1</th><td>0.0000000</td><td>0.2631579</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & wealth1 & taste1\\\\\n",
       "\\hline\n",
       "\twealth1 & 0.1684211 & 0.0000000\\\\\n",
       "\ttaste1 & 0.0000000 & 0.2631579\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | wealth1 | taste1 |\n",
       "|---|---|---|\n",
       "| wealth1 | 0.1684211 | 0.0000000 |\n",
       "| taste1 | 0.0000000 | 0.2631579 |\n",
       "\n"
      ],
      "text/plain": [
       "        wealth1   taste1   \n",
       "wealth1 0.1684211 0.0000000\n",
       "taste1  0.0000000 0.2631579"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cov(sit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Standard deviations (1, .., p=2):\n",
       "[1] 0.5129892 0.4103913\n",
       "\n",
       "Rotation (n x k) = (2 x 2):\n",
       "        PC1 PC2\n",
       "wealth1   0   1\n",
       "taste1   -1   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prcomp(sit1, center=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.808"
      ],
      "text/latex": [
       "0.808"
      ],
      "text/markdown": [
       "0.808"
      ],
      "text/plain": [
       "[1] 0.808"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_w <- runif(2000, min = 0, max = 1)\n",
    "w <- as.integer(draw_w > 4/20)\n",
    "mean(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.5"
      ],
      "text/latex": [
       "0.5"
      ],
      "text/markdown": [
       "0.5"
      ],
      "text/plain": [
       "[1] 0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_t <- runif(2000, min=0, max=1)\n",
    "t <- as.integer(draw_t > 0.5)\n",
    "mean(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>w</th><th scope=col>t</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>w</th><td>0.155213607</td><td>0.002501251</td></tr>\n",
       "\t<tr><th scope=row>t</th><td>0.002501251</td><td>0.250125063</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & w & t\\\\\n",
       "\\hline\n",
       "\tw & 0.155213607 & 0.002501251\\\\\n",
       "\tt & 0.002501251 & 0.250125063\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | w | t |\n",
       "|---|---|---|\n",
       "| w | 0.155213607 | 0.002501251 |\n",
       "| t | 0.002501251 | 0.250125063 |\n",
       "\n"
      ],
      "text/plain": [
       "  w           t          \n",
       "w 0.155213607 0.002501251\n",
       "t 0.002501251 0.250125063"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x <- cbind(w, t)\n",
    "cov(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Standard deviations (1, .., p=2):\n",
       "[1] 0.5001909 0.3938880\n",
       "\n",
       "Rotation (n x k) = (2 x 2):\n",
       "         PC1         PC2\n",
       "w 0.02632611 -0.99965341\n",
       "t 0.99965341  0.02632611"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prcomp(x, center=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of the Empirical Setting\n",
    "\n",
    "Over the last two decades, economic research has increasingly emphasised the role of personality traits for labour market success. One personality traits that has especeially been indentified as crucial in this context is *locus of control* (loc). This concept captures  the extent to which an\n",
    "individual believes that her life can be shaped by her own actions and decisions (internal loc) or is instead contingent on outside factors beyond her control and thus on fate and luck (external loc). It stands to reason that a more internal loc may be positively associated with outcomes such as schooling and wages, as the individual feels a higher degree of control over her own course of life which may induce her to spend more effort. There is empirical evidence showing that an internal loc is predictive of wages even after controlling for education.\n",
    "\n",
    "The crucial aspect when trying to identify the relationship between loc and wages is measurement. Personality traits cannot directly be measured, they are latent variables. However, psychologists developed \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Standardized Variables\n",
    "To begin with, it is common practice to standardize the explanatory variables. This is because of an undesirable feature that PCA has when units of measurement are changed for at least one explanatory variables. A change in the units of measurement of a variable also affects its variance - and as the principal components are constructed such that they capture the greatest possible variability of the original variables, it follows that the principal components would also change. The standardization is conducted as follows:\n",
    "\n",
    "$$ \\tilde{x}_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_j} \\text{,}$$\n",
    "\n",
    "where $\\bar{x}_j$ denotes the sample mean of the $j$th explanatory variable across the $N$ individuals and $s_j$ denotes the corresponding standard deviation. We are then interested in obtaining linear combinations $Z_1, \\dots, Z_M$ of the standardized variables, where $M \\leq P$. Those linear combinations are referred to as prinicpal components and they are ordered according to their variance in a descending fashion, thus the first principal component $Z_1$ is the linear combination of the standardized explanatory variables,\n",
    "\n",
    "$$ Z_1 = \\phi_{11} \\tilde{X}_1 + \\phi_{12} \\tilde{X}_2 + \\dots + \\phi_{1P} \\tilde{X}_P = \\sum_{j=1}^P \\phi_{1j} \\tilde{X}_j = \\tilde{\\pmb X} \\phi_1 \\, \\text{ ,}$$\n",
    "\n",
    "that yields the highest variance. The scalars $\\phi_{11}, \\phi_{12}, \\dots, \\phi_{1P}$ are referred to as *loadings* of the first principal component and $\\phi_1$ denotes the corresponding column vector $\\phi_1 = (\\phi_{11} \\, \\phi_{12}  \\, \\dots \\,  \\phi_{1P})^T$. \n",
    "\n",
    "To derive the actual loadings of the first principal component, we have to note that any such linear combination has its variance given by $Var(\\tilde{\\pmb X} \\phi)  = \\phi^T \\pmb S \\phi$, where $\\pmb S$ denotes the sample covariance matrix of the standardized variables (which, due to the standardization, is equivalent to the correlation matrix of the original explanatory variables). Thus, the vector $\\phi_1$ must be obtained, which maximises the quadratic form $\\phi^T \\pmb S \\phi$. To ensure a well-defined solution to this problem, $\\phi$ is required to be a unit-norm vector, i.e. $\\phi^T \\phi = 1$. Put together, the constraint maximisation problem can be written as \n",
    "\n",
    "$$\\max_{\\phi} \\phi^T \\pmb S \\phi - \\lambda (\\phi^T \\phi -1)$$\n",
    "\n",
    "Solving this problem yields:\n",
    "\n",
    "$$ \\pmb S \\phi - \\lambda \\phi = \\pmb 0 \\iff  \\pmb S \\phi = \\lambda \\phi$$\n",
    "\n",
    "Thus, $\\phi$ is an eigenvector of the covariance matrix $\\pmb S$ and $\\lambda$ the corresponding eigenvalue. Since we are interested in the linear combination with the largest variance  $Z_1$, the respective loadings vector $\\phi_1$ (i.e. the eigenvector) is identified by the largest eigenvalue $\\lambda_1$ as \n",
    "\n",
    "$$ Var(\\tilde{\\pmb X} \\phi)  = \\phi^T \\pmb S \\phi = \\lambda \\phi^T \\phi = \\lambda \\, \\text{ ,}$$\n",
    "\n",
    "using the solution of the maximisation problem. Note that this solution of the maximisation problem is unaffected if the eigenvectors $\\phi$ would be multiplied by (-1), so the actual signs of the loadings are meaningless - merely their respective magnitudes as well as their sign patterns matter. \n",
    "\n",
    "As $\\pmb S$ is a symmetric $P \\times P$ matrix, a total number of $P$ eigenvalues can be obtained. The corresponding eigenvectors $\\phi_k$ with $k=1, \\dots, P$ can be required to be orthogonal to each other, which is ensured when they satify $\\phi_{k}^T \\phi_{k'} = 1$ for $k=k'$ and zero otherwise. It can be shown that the remaining $2, \\dots, P$ linear combinations can be found as solutions to the problem of successively maximizing the variance subject to the constraint of uncorrelatedness with previously obtained linear combinations.\n",
    "\n",
    "In consequence, we are able to obtain $P$ linear combinations, i.e. principal components, of the $P$ standardized explanatory variables - and all of these linear combinations are uncorrelated. Further, they are arranged in descending order according to their variance, which implies that the variablility of the original variables is concentrated within the first couple of principal components. Dimensionality reduction now means, that not all of the principal components are used (otherwise no dimensionality reduction would have been achieved) but instead only the principal components up to $M < P$ are utilised. In a regression framework, this implies conducting Principal Component Regression (PCR): instead of regressing the outcome variable $Y$ on the original explanatory variables $\\pmb X$, we regress it on a number of principal components:\n",
    "\n",
    "$$ Y_i = \\gamma_0 + \\gamma_1 Z_{1i} + \\dots + \\gamma_M Z_{Mi} \\, \\text{ ,}$$\n",
    "\n",
    "The crucial assumption thereby is that the directions in which the (standardized) original variables have the most variability are also the directions that are associated with the outcome variable $Y$. The word \"direction\" is used as the vectors of principal component loadings, $\\phi$, characterise a direction in a multi-dimensional space. Given that this assumption holds, using only $M$ linear combinations that contain most of the original statistical information improves upon the overfitting problem if $M << P$. Referring back to the variance-bias trade-off, we accept that a small part of the original statistical information is not used (i.e. introducting bias) while achieving a reduction of the variance of our predictions when being applied to different data sets of the same kind which is likely to overcompensate for the introduced bias.  Further, interpretatibility of the model is increased as only a lower number of variables is included - which are also uncorrelated, so multicollinearity is no longer an issue.\n",
    "\n",
    "The questions remains, how many principal components should be included in the regression model. A helpful concept in this context is the *proportion of variance explained* (PVE). As already stated, we intend to reduce the dimensionality of the data while preserving as much as possible of the original variability. So the PVE states how much of the total variance, is captured by an individual principal component. As we standardized the explanatory variables, their variance is always equal to one, so the total variance is equal to $\\sum{j=1}{P} \\tilde{X}_j = P$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>3.0</td><td>0.5</td></tr>\n",
       "\t<tr><td>0.5</td><td>2.0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{ll}\n",
       "\t 3.0 & 0.5\\\\\n",
       "\t 0.5 & 2.0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 3.0 | 0.5 |\n",
       "| 0.5 | 2.0 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2]\n",
       "[1,] 3.0  0.5 \n",
       "[2,] 0.5  2.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td> 3.012768784</td><td> 0.45441131 </td><td>-0.005107013</td></tr>\n",
       "\t<tr><td> 0.454411312</td><td> 1.93275691 </td><td>-0.048279994</td></tr>\n",
       "\t<tr><td>-0.005107013</td><td>-0.04827999 </td><td> 1.018541633</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lll}\n",
       "\t  3.012768784 &  0.45441131  & -0.005107013\\\\\n",
       "\t  0.454411312 &  1.93275691  & -0.048279994\\\\\n",
       "\t -0.005107013 & -0.04827999  &  1.018541633\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  3.012768784 |  0.45441131  | -0.005107013 |\n",
       "|  0.454411312 |  1.93275691  | -0.048279994 |\n",
       "| -0.005107013 | -0.04827999  |  1.018541633 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]         [,2]        [,3]        \n",
       "[1,]  3.012768784  0.45441131 -0.005107013\n",
       "[2,]  0.454411312  1.93275691 -0.048279994\n",
       "[3,] -0.005107013 -0.04827999  1.018541633"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Importance of components:\n",
       "                         PC1    PC2    PC3\n",
       "Standard deviation     1.783 1.3302 1.0079\n",
       "Proportion of Variance 0.533 0.2967 0.1703\n",
       "Cumulative Proportion  0.533 0.8297 1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up\n",
    "\n",
    "library(mvtnorm)\n",
    "\n",
    "sigma <- diag(seq(3, 2))\n",
    "sigma[2, 1] = 0.5\n",
    "sigma[1, 2] = 0.5\n",
    "sigma\n",
    "\n",
    "X <- rmvnorm(3000, rep(0, 2), sigma)\n",
    "X <- cbind(X, rnorm(3000))\n",
    "\n",
    "cov(X)\n",
    "summary(prcomp(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td> 5.06953665</td><td>-0.05254272</td><td>-0.12774616</td><td>-0.11042395</td><td>-0.06478961</td></tr>\n",
       "\t<tr><td>-0.05254272</td><td> 4.16116633</td><td> 0.13416757</td><td> 0.12995139</td><td>-0.05227279</td></tr>\n",
       "\t<tr><td>-0.12774616</td><td> 0.13416757</td><td> 2.87919061</td><td> 0.07581436</td><td> 0.03986043</td></tr>\n",
       "\t<tr><td>-0.11042395</td><td> 0.12995139</td><td> 0.07581436</td><td> 2.01414017</td><td>-0.03951104</td></tr>\n",
       "\t<tr><td>-0.06478961</td><td>-0.05227279</td><td> 0.03986043</td><td>-0.03951104</td><td> 0.97085013</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lllll}\n",
       "\t  5.06953665 & -0.05254272 & -0.12774616 & -0.11042395 & -0.06478961\\\\\n",
       "\t -0.05254272 &  4.16116633 &  0.13416757 &  0.12995139 & -0.05227279\\\\\n",
       "\t -0.12774616 &  0.13416757 &  2.87919061 &  0.07581436 &  0.03986043\\\\\n",
       "\t -0.11042395 &  0.12995139 &  0.07581436 &  2.01414017 & -0.03951104\\\\\n",
       "\t -0.06478961 & -0.05227279 &  0.03986043 & -0.03951104 &  0.97085013\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  5.06953665 | -0.05254272 | -0.12774616 | -0.11042395 | -0.06478961 |\n",
       "| -0.05254272 |  4.16116633 |  0.13416757 |  0.12995139 | -0.05227279 |\n",
       "| -0.12774616 |  0.13416757 |  2.87919061 |  0.07581436 |  0.03986043 |\n",
       "| -0.11042395 |  0.12995139 |  0.07581436 |  2.01414017 | -0.03951104 |\n",
       "| -0.06478961 | -0.05227279 |  0.03986043 | -0.03951104 |  0.97085013 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]        [,2]        [,3]        [,4]        [,5]       \n",
       "[1,]  5.06953665 -0.05254272 -0.12774616 -0.11042395 -0.06478961\n",
       "[2,] -0.05254272  4.16116633  0.13416757  0.12995139 -0.05227279\n",
       "[3,] -0.12774616  0.13416757  2.87919061  0.07581436  0.03986043\n",
       "[4,] -0.11042395  0.12995139  0.07581436  2.01414017 -0.03951104\n",
       "[5,] -0.06478961 -0.05227279  0.03986043 -0.03951104  0.97085013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Standard deviations (1, .., p=5):\n",
       "[1] 2.2554053 2.0448337 1.6921449 1.4176788 0.9831186\n",
       "\n",
       "Rotation (n x k) = (5 x 5):\n",
       "             PC1         PC2         PC3         PC4         PC5\n",
       "[1,] -0.99459500  0.07838311 -0.05859950  0.03060646  0.01631630\n",
       "[2,]  0.07024644  0.99004030  0.10633482 -0.05765699  0.01594456\n",
       "[3,]  0.06343909  0.09794736 -0.99021016 -0.07323910 -0.02239789\n",
       "[4,]  0.03996011  0.06151417 -0.06578054  0.99437675  0.03881995\n",
       "[5,]  0.01499669 -0.01728071 -0.02039016 -0.03987258  0.99873465"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Importance of components:\n",
       "                          PC1    PC2    PC3   PC4     PC5\n",
       "Standard deviation     2.2554 2.0448 1.6921 1.418 0.98312\n",
       "Proportion of Variance 0.3367 0.2768 0.1895 0.133 0.06397\n",
       "Cumulative Proportion  0.3367 0.6135 0.8030 0.936 1.00000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test <- rmvnorm(2000, rep(0, 5), sigma=diag(5:1))\n",
    "cov(test)\n",
    "\n",
    "pca <- prcomp(test, center=FALSE)\n",
    "pca\n",
    "summary(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td> 1.00000000</td><td>-0.01143986</td><td>-0.03343709</td><td>-0.03455686</td><td>-0.02920419</td></tr>\n",
       "\t<tr><td>-0.01143986</td><td> 1.00000000</td><td> 0.03876188</td><td> 0.04488782</td><td>-0.02600711</td></tr>\n",
       "\t<tr><td>-0.03343709</td><td> 0.03876188</td><td> 1.00000000</td><td> 0.03148266</td><td> 0.02384134</td></tr>\n",
       "\t<tr><td>-0.03455686</td><td> 0.04488782</td><td> 0.03148266</td><td> 1.00000000</td><td>-0.02825515</td></tr>\n",
       "\t<tr><td>-0.02920419</td><td>-0.02600711</td><td> 0.02384134</td><td>-0.02825515</td><td> 1.00000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lllll}\n",
       "\t  1.00000000 & -0.01143986 & -0.03343709 & -0.03455686 & -0.02920419\\\\\n",
       "\t -0.01143986 &  1.00000000 &  0.03876188 &  0.04488782 & -0.02600711\\\\\n",
       "\t -0.03343709 &  0.03876188 &  1.00000000 &  0.03148266 &  0.02384134\\\\\n",
       "\t -0.03455686 &  0.04488782 &  0.03148266 &  1.00000000 & -0.02825515\\\\\n",
       "\t -0.02920419 & -0.02600711 &  0.02384134 & -0.02825515 &  1.00000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  1.00000000 | -0.01143986 | -0.03343709 | -0.03455686 | -0.02920419 |\n",
       "| -0.01143986 |  1.00000000 |  0.03876188 |  0.04488782 | -0.02600711 |\n",
       "| -0.03343709 |  0.03876188 |  1.00000000 |  0.03148266 |  0.02384134 |\n",
       "| -0.03455686 |  0.04488782 |  0.03148266 |  1.00000000 | -0.02825515 |\n",
       "| -0.02920419 | -0.02600711 |  0.02384134 | -0.02825515 |  1.00000000 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]        [,2]        [,3]        [,4]        [,5]       \n",
       "[1,]  1.00000000 -0.01143986 -0.03343709 -0.03455686 -0.02920419\n",
       "[2,] -0.01143986  1.00000000  0.03876188  0.04488782 -0.02600711\n",
       "[3,] -0.03343709  0.03876188  1.00000000  0.03148266  0.02384134\n",
       "[4,] -0.03455686  0.04488782  0.03148266  1.00000000 -0.02825515\n",
       "[5,] -0.02920419 -0.02600711  0.02384134 -0.02825515  1.00000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cov2cor(cov(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
